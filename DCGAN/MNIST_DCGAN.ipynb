{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1511.06434.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/abhinav/anaconda3/envs/dl_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, MaxPooling2D, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        self.latent_dim = 100\n",
    "        self.rows = 28\n",
    "        self.cols = 28\n",
    "        self.channels = 1\n",
    "        \n",
    "        self.sgd = SGD(lr=0.0005, decay=2e-5, momentum=0.9, nesterov=True)\n",
    "        self.adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-4)\n",
    "\n",
    "        self.discriminator = self.discriminator_model()\n",
    "        self.discriminator.compile(loss = 'binary_crossentropy', optimizer = self.adam, metrics = [\"accuracy\"])\n",
    "        \n",
    "        self.generator = self.generator_model()\n",
    "        \n",
    "        generator_input = Input(shape=(self.latent_dim,))\n",
    "        generator_output = self.generator(generator_input)\n",
    "        self.discriminator.trainable = False\n",
    "        discriminator_output = self.discriminator(generator_output)\n",
    "        \n",
    "        self.combined_model = Model(generator_input, discriminator_output)\n",
    "        self.combined_model.compile(loss = 'binary_crossentropy', optimizer = self.adam, metrics = [\"accuracy\"])\n",
    "        \n",
    "    def generator_model(self):\n",
    "\n",
    "        gen_input = Input(shape=(self.latent_dim,))\n",
    "        dense_1 = Dense(1024, activation=\"tanh\")(gen_input)\n",
    "        dense_1 = Dense(128 * 7 * 7, activation=\"tanh\")(dense_1)\n",
    "        dense_1 = BatchNormalization()(dense_1)\n",
    "        dense_reshape = Reshape((7, 7, 128))(dense_1)\n",
    "        \n",
    "        upsample_1 = UpSampling2D(size = 2)(dense_reshape)\n",
    "        conv_1 = Conv2D(64, kernel_size=5, padding=\"same\", activation='tanh')(upsample_1)\n",
    "        conv_1 = BatchNormalization()(conv_1)\n",
    "        \n",
    "        upsample_2 = UpSampling2D(size = 2)(conv_1)\n",
    "        out = Conv2D(self.channels, kernel_size=5, padding=\"same\", activation='tanh')(upsample_2)\n",
    "        \n",
    "        model = Model(gen_input, out)\n",
    "\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        \n",
    "        dis_input = Input(shape=(self.rows, self.cols, self.channels))\n",
    "        \n",
    "        conv_1 = Conv2D(64, kernel_size=5, padding=\"same\", activation=\"tanh\")(dis_input)\n",
    "        pool_1 = MaxPooling2D(pool_size=(2,2))(conv_1)\n",
    "        \n",
    "        conv_2 = Conv2D(128, kernel_size=5, padding=\"same\", activation=\"tanh\")(pool_1)\n",
    "        pool_2 = MaxPooling2D(pool_size=(2,2))(conv_2)\n",
    "        \n",
    "        flatten = Flatten()(pool_2)\n",
    "        flatten = Dense(1024, activation='tanh')(flatten)\n",
    "        \n",
    "        out = Dense(1, activation=\"sigmoid\")(flatten)\n",
    "        \n",
    "        model = Model(dis_input, out)\n",
    "        \n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def train(self, epochs, batch_size=128, save_interval=100):\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        print (X_train.shape)\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for iter_count in range(X_train.shape[0]//batch_size):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = self.combined_model.train_on_batch(noise, valid)\n",
    "\n",
    "                if iter_count % save_interval == 0:\n",
    "                    print (\"%d %d [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%]\" % (i, iter_count, d_loss[0], 100*d_loss[1], g_loss[0], 100*g_loss[1]))\n",
    "            \n",
    "            self.save_imgs(i)                \n",
    "            self.combined_model.save('combined_model.h5')\n",
    "            \n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              6423552   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 6,631,169\n",
      "Trainable params: 6,631,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6272)              6428800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 6,764,033\n",
      "Trainable params: 6,751,361\n",
      "Non-trainable params: 12,672\n",
      "_________________________________________________________________\n",
      "None\n",
      "(60000, 28, 28, 1)\n",
      "0 0 [D loss: 0.817405, acc.: 1.17%] [G loss: 0.373920, acc.: 100.00%]\n",
      "0 50 [D loss: 0.025297, acc.: 99.22%] [G loss: 7.284903, acc.: 0.78%]\n",
      "0 100 [D loss: 0.066007, acc.: 98.83%] [G loss: 6.295187, acc.: 1.56%]\n",
      "0 150 [D loss: 0.566617, acc.: 77.34%] [G loss: 8.932235, acc.: 17.97%]\n",
      "0 200 [D loss: 0.176036, acc.: 91.80%] [G loss: 5.145351, acc.: 1.56%]\n",
      "0 250 [D loss: 0.411869, acc.: 84.38%] [G loss: 7.850914, acc.: 11.72%]\n",
      "0 300 [D loss: 0.423459, acc.: 79.69%] [G loss: 9.002529, acc.: 12.50%]\n",
      "0 350 [D loss: 0.188236, acc.: 93.36%] [G loss: 6.766288, acc.: 6.25%]\n",
      "0 400 [D loss: 0.283611, acc.: 87.11%] [G loss: 7.972885, acc.: 7.81%]\n",
      "0 450 [D loss: 0.104684, acc.: 96.88%] [G loss: 7.240027, acc.: 3.12%]\n",
      "1 0 [D loss: 0.123937, acc.: 94.92%] [G loss: 6.594011, acc.: 1.56%]\n",
      "1 50 [D loss: 0.222802, acc.: 91.80%] [G loss: 3.676640, acc.: 2.34%]\n",
      "1 100 [D loss: 0.237183, acc.: 92.19%] [G loss: 2.306152, acc.: 5.47%]\n",
      "1 150 [D loss: 0.297878, acc.: 91.02%] [G loss: 1.760514, acc.: 8.59%]\n",
      "1 200 [D loss: 0.280401, acc.: 91.41%] [G loss: 1.526698, acc.: 10.16%]\n",
      "1 250 [D loss: 0.329021, acc.: 89.45%] [G loss: 1.360442, acc.: 12.50%]\n",
      "1 300 [D loss: 0.276103, acc.: 94.14%] [G loss: 1.312990, acc.: 7.03%]\n",
      "1 350 [D loss: 0.312939, acc.: 92.19%] [G loss: 1.504457, acc.: 4.69%]\n",
      "1 400 [D loss: 0.284165, acc.: 96.48%] [G loss: 1.561142, acc.: 3.12%]\n",
      "1 450 [D loss: 0.335132, acc.: 92.58%] [G loss: 1.437905, acc.: 0.78%]\n",
      "2 0 [D loss: 0.324980, acc.: 92.97%] [G loss: 1.398837, acc.: 3.91%]\n",
      "2 50 [D loss: 0.339098, acc.: 91.80%] [G loss: 1.451361, acc.: 4.69%]\n",
      "2 100 [D loss: 0.329113, acc.: 91.02%] [G loss: 1.456475, acc.: 4.69%]\n",
      "2 150 [D loss: 0.283384, acc.: 94.53%] [G loss: 1.707258, acc.: 1.56%]\n",
      "2 200 [D loss: 0.330611, acc.: 89.45%] [G loss: 1.683857, acc.: 3.12%]\n",
      "2 250 [D loss: 0.328250, acc.: 89.84%] [G loss: 1.708633, acc.: 4.69%]\n",
      "2 300 [D loss: 0.292600, acc.: 92.19%] [G loss: 1.707869, acc.: 3.91%]\n",
      "2 350 [D loss: 0.284736, acc.: 95.31%] [G loss: 1.866449, acc.: 1.56%]\n",
      "2 400 [D loss: 0.331013, acc.: 87.11%] [G loss: 1.633008, acc.: 5.47%]\n",
      "2 450 [D loss: 0.306685, acc.: 92.58%] [G loss: 1.607761, acc.: 3.91%]\n",
      "3 0 [D loss: 0.323578, acc.: 91.02%] [G loss: 1.714025, acc.: 4.69%]\n",
      "3 50 [D loss: 0.340327, acc.: 90.62%] [G loss: 1.657975, acc.: 2.34%]\n",
      "3 100 [D loss: 0.355803, acc.: 88.67%] [G loss: 1.593082, acc.: 3.12%]\n",
      "3 150 [D loss: 0.311135, acc.: 90.23%] [G loss: 1.678481, acc.: 3.12%]\n",
      "3 200 [D loss: 0.354145, acc.: 86.72%] [G loss: 1.734706, acc.: 5.47%]\n",
      "3 250 [D loss: 0.361974, acc.: 87.50%] [G loss: 1.812001, acc.: 4.69%]\n",
      "3 300 [D loss: 0.334948, acc.: 88.28%] [G loss: 1.824110, acc.: 3.91%]\n",
      "3 350 [D loss: 0.304202, acc.: 89.84%] [G loss: 1.828536, acc.: 3.91%]\n",
      "3 400 [D loss: 0.303243, acc.: 89.45%] [G loss: 1.785592, acc.: 2.34%]\n",
      "3 450 [D loss: 0.326060, acc.: 87.50%] [G loss: 1.923326, acc.: 3.12%]\n",
      "4 0 [D loss: 0.340438, acc.: 87.11%] [G loss: 1.922150, acc.: 2.34%]\n",
      "4 50 [D loss: 0.352390, acc.: 86.72%] [G loss: 1.734412, acc.: 5.47%]\n",
      "4 100 [D loss: 0.375547, acc.: 88.28%] [G loss: 1.760660, acc.: 3.91%]\n",
      "4 150 [D loss: 0.281574, acc.: 90.23%] [G loss: 2.062553, acc.: 1.56%]\n",
      "4 200 [D loss: 0.312930, acc.: 90.62%] [G loss: 1.811102, acc.: 3.12%]\n",
      "4 250 [D loss: 0.288696, acc.: 90.62%] [G loss: 1.873622, acc.: 3.91%]\n",
      "4 300 [D loss: 0.320520, acc.: 88.28%] [G loss: 1.762931, acc.: 6.25%]\n",
      "4 350 [D loss: 0.303189, acc.: 90.23%] [G loss: 1.942826, acc.: 6.25%]\n",
      "4 400 [D loss: 0.311727, acc.: 89.45%] [G loss: 1.832073, acc.: 4.69%]\n",
      "4 450 [D loss: 0.284346, acc.: 91.02%] [G loss: 1.956123, acc.: 5.47%]\n",
      "5 0 [D loss: 0.307473, acc.: 90.23%] [G loss: 1.918312, acc.: 5.47%]\n",
      "5 50 [D loss: 0.257753, acc.: 90.62%] [G loss: 2.049751, acc.: 3.12%]\n",
      "5 100 [D loss: 0.318076, acc.: 89.45%] [G loss: 1.838006, acc.: 3.12%]\n",
      "5 150 [D loss: 0.335430, acc.: 86.33%] [G loss: 1.698153, acc.: 8.59%]\n",
      "5 200 [D loss: 0.328148, acc.: 88.28%] [G loss: 1.676697, acc.: 10.16%]\n",
      "5 250 [D loss: 0.348558, acc.: 85.94%] [G loss: 1.871664, acc.: 6.25%]\n",
      "5 300 [D loss: 0.311746, acc.: 88.67%] [G loss: 1.842951, acc.: 5.47%]\n",
      "5 350 [D loss: 0.319322, acc.: 88.28%] [G loss: 1.864852, acc.: 3.12%]\n",
      "5 400 [D loss: 0.312261, acc.: 89.84%] [G loss: 1.884833, acc.: 2.34%]\n",
      "5 450 [D loss: 0.321122, acc.: 88.67%] [G loss: 1.788933, acc.: 3.91%]\n",
      "6 0 [D loss: 0.328218, acc.: 87.50%] [G loss: 1.856785, acc.: 5.47%]\n",
      "6 50 [D loss: 0.343023, acc.: 86.72%] [G loss: 1.775768, acc.: 7.81%]\n",
      "6 100 [D loss: 0.340078, acc.: 86.72%] [G loss: 1.695517, acc.: 7.03%]\n",
      "6 150 [D loss: 0.337365, acc.: 86.72%] [G loss: 1.828125, acc.: 5.47%]\n",
      "6 200 [D loss: 0.314305, acc.: 90.62%] [G loss: 1.632524, acc.: 5.47%]\n",
      "6 250 [D loss: 0.337323, acc.: 86.72%] [G loss: 1.756094, acc.: 6.25%]\n",
      "6 300 [D loss: 0.322990, acc.: 87.89%] [G loss: 1.717514, acc.: 5.47%]\n",
      "6 350 [D loss: 0.390798, acc.: 80.47%] [G loss: 1.657035, acc.: 9.38%]\n",
      "6 400 [D loss: 0.358363, acc.: 88.67%] [G loss: 1.801526, acc.: 5.47%]\n",
      "6 450 [D loss: 0.327706, acc.: 89.84%] [G loss: 1.908332, acc.: 2.34%]\n",
      "7 0 [D loss: 0.388205, acc.: 83.59%] [G loss: 1.602478, acc.: 7.03%]\n",
      "7 50 [D loss: 0.350573, acc.: 83.20%] [G loss: 1.912426, acc.: 3.91%]\n",
      "7 100 [D loss: 0.320236, acc.: 87.89%] [G loss: 1.738976, acc.: 7.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 150 [D loss: 0.371119, acc.: 84.77%] [G loss: 1.634735, acc.: 10.16%]\n",
      "7 200 [D loss: 0.298690, acc.: 91.41%] [G loss: 1.952257, acc.: 4.69%]\n",
      "7 250 [D loss: 0.376679, acc.: 83.98%] [G loss: 1.655778, acc.: 12.50%]\n",
      "7 300 [D loss: 0.390392, acc.: 82.81%] [G loss: 1.654531, acc.: 7.81%]\n",
      "7 350 [D loss: 0.364750, acc.: 84.38%] [G loss: 1.729358, acc.: 10.16%]\n",
      "7 400 [D loss: 0.348464, acc.: 86.33%] [G loss: 1.760110, acc.: 6.25%]\n",
      "7 450 [D loss: 0.412649, acc.: 82.81%] [G loss: 1.623608, acc.: 10.94%]\n",
      "8 0 [D loss: 0.361467, acc.: 83.59%] [G loss: 1.721581, acc.: 8.59%]\n",
      "8 50 [D loss: 0.340875, acc.: 88.28%] [G loss: 1.756008, acc.: 6.25%]\n",
      "8 100 [D loss: 0.357502, acc.: 85.55%] [G loss: 1.708528, acc.: 8.59%]\n",
      "8 150 [D loss: 0.406959, acc.: 82.03%] [G loss: 1.545867, acc.: 13.28%]\n",
      "8 200 [D loss: 0.332079, acc.: 90.23%] [G loss: 1.606299, acc.: 7.81%]\n",
      "8 250 [D loss: 0.392856, acc.: 82.42%] [G loss: 1.687578, acc.: 10.16%]\n",
      "8 300 [D loss: 0.359290, acc.: 86.72%] [G loss: 1.738871, acc.: 8.59%]\n",
      "8 350 [D loss: 0.373927, acc.: 83.98%] [G loss: 1.612084, acc.: 10.16%]\n",
      "8 400 [D loss: 0.443065, acc.: 80.08%] [G loss: 1.484492, acc.: 12.50%]\n",
      "8 450 [D loss: 0.380249, acc.: 83.98%] [G loss: 1.659130, acc.: 12.50%]\n",
      "9 0 [D loss: 0.369489, acc.: 84.77%] [G loss: 1.779936, acc.: 7.81%]\n",
      "9 50 [D loss: 0.429866, acc.: 81.64%] [G loss: 1.538882, acc.: 8.59%]\n",
      "9 100 [D loss: 0.444085, acc.: 80.47%] [G loss: 1.492414, acc.: 12.50%]\n",
      "9 150 [D loss: 0.345660, acc.: 86.72%] [G loss: 1.701122, acc.: 10.94%]\n",
      "9 200 [D loss: 0.479689, acc.: 77.34%] [G loss: 1.348113, acc.: 11.72%]\n",
      "9 250 [D loss: 0.419563, acc.: 81.25%] [G loss: 1.616092, acc.: 8.59%]\n",
      "9 300 [D loss: 0.372385, acc.: 85.16%] [G loss: 1.794532, acc.: 4.69%]\n",
      "9 350 [D loss: 0.393579, acc.: 82.42%] [G loss: 1.642230, acc.: 8.59%]\n",
      "9 400 [D loss: 0.438364, acc.: 78.91%] [G loss: 1.509661, acc.: 10.94%]\n",
      "9 450 [D loss: 0.405219, acc.: 82.03%] [G loss: 1.469739, acc.: 14.06%]\n",
      "10 0 [D loss: 0.439248, acc.: 80.47%] [G loss: 1.527178, acc.: 13.28%]\n",
      "10 50 [D loss: 0.431693, acc.: 78.91%] [G loss: 1.528665, acc.: 12.50%]\n",
      "10 100 [D loss: 0.411953, acc.: 84.77%] [G loss: 1.721089, acc.: 7.03%]\n",
      "10 150 [D loss: 0.444094, acc.: 78.52%] [G loss: 1.445376, acc.: 12.50%]\n",
      "10 200 [D loss: 0.460005, acc.: 74.61%] [G loss: 1.484749, acc.: 16.41%]\n",
      "10 250 [D loss: 0.438367, acc.: 78.91%] [G loss: 1.618948, acc.: 11.72%]\n",
      "10 300 [D loss: 0.444068, acc.: 79.69%] [G loss: 1.460960, acc.: 14.84%]\n",
      "10 350 [D loss: 0.387806, acc.: 85.16%] [G loss: 1.666779, acc.: 9.38%]\n",
      "10 400 [D loss: 0.437464, acc.: 80.86%] [G loss: 1.605916, acc.: 8.59%]\n",
      "10 450 [D loss: 0.451392, acc.: 78.91%] [G loss: 1.484514, acc.: 13.28%]\n",
      "11 0 [D loss: 0.480245, acc.: 78.12%] [G loss: 1.517249, acc.: 10.16%]\n",
      "11 50 [D loss: 0.424490, acc.: 82.03%] [G loss: 1.536066, acc.: 11.72%]\n",
      "11 100 [D loss: 0.390403, acc.: 85.94%] [G loss: 1.472951, acc.: 9.38%]\n",
      "11 150 [D loss: 0.437379, acc.: 81.64%] [G loss: 1.519517, acc.: 8.59%]\n",
      "11 200 [D loss: 0.459175, acc.: 79.69%] [G loss: 1.377175, acc.: 17.19%]\n",
      "11 250 [D loss: 0.411150, acc.: 82.81%] [G loss: 1.535686, acc.: 10.16%]\n",
      "11 300 [D loss: 0.448571, acc.: 79.69%] [G loss: 1.481030, acc.: 11.72%]\n",
      "11 350 [D loss: 0.450552, acc.: 80.47%] [G loss: 1.526215, acc.: 11.72%]\n",
      "11 400 [D loss: 0.445435, acc.: 79.30%] [G loss: 1.536304, acc.: 8.59%]\n",
      "11 450 [D loss: 0.510441, acc.: 79.30%] [G loss: 1.441266, acc.: 11.72%]\n",
      "12 0 [D loss: 0.444731, acc.: 79.30%] [G loss: 1.560436, acc.: 14.06%]\n",
      "12 50 [D loss: 0.452195, acc.: 80.08%] [G loss: 1.390896, acc.: 9.38%]\n",
      "12 100 [D loss: 0.454814, acc.: 80.47%] [G loss: 1.485357, acc.: 11.72%]\n",
      "12 150 [D loss: 0.449158, acc.: 79.69%] [G loss: 1.489006, acc.: 14.84%]\n",
      "12 200 [D loss: 0.480036, acc.: 78.91%] [G loss: 1.367207, acc.: 17.19%]\n",
      "12 250 [D loss: 0.465928, acc.: 79.30%] [G loss: 1.371826, acc.: 14.06%]\n",
      "12 300 [D loss: 0.494373, acc.: 80.08%] [G loss: 1.308535, acc.: 17.19%]\n",
      "12 350 [D loss: 0.468426, acc.: 76.95%] [G loss: 1.408966, acc.: 17.97%]\n",
      "12 400 [D loss: 0.428794, acc.: 81.25%] [G loss: 1.541267, acc.: 14.84%]\n",
      "12 450 [D loss: 0.453518, acc.: 77.34%] [G loss: 1.367573, acc.: 15.62%]\n",
      "13 0 [D loss: 0.425265, acc.: 82.03%] [G loss: 1.496450, acc.: 10.94%]\n",
      "13 50 [D loss: 0.494933, acc.: 79.30%] [G loss: 1.444329, acc.: 14.84%]\n",
      "13 100 [D loss: 0.434150, acc.: 82.03%] [G loss: 1.412025, acc.: 12.50%]\n",
      "13 150 [D loss: 0.455707, acc.: 79.30%] [G loss: 1.334915, acc.: 17.97%]\n",
      "13 200 [D loss: 0.475182, acc.: 78.52%] [G loss: 1.356470, acc.: 18.75%]\n",
      "13 250 [D loss: 0.445162, acc.: 82.81%] [G loss: 1.453729, acc.: 9.38%]\n",
      "13 300 [D loss: 0.463887, acc.: 78.12%] [G loss: 1.391361, acc.: 14.06%]\n",
      "13 350 [D loss: 0.475694, acc.: 78.52%] [G loss: 1.319185, acc.: 14.06%]\n",
      "13 400 [D loss: 0.442575, acc.: 81.25%] [G loss: 1.397818, acc.: 10.94%]\n",
      "13 450 [D loss: 0.460050, acc.: 78.12%] [G loss: 1.333144, acc.: 19.53%]\n",
      "14 0 [D loss: 0.474910, acc.: 78.12%] [G loss: 1.316080, acc.: 16.41%]\n",
      "14 50 [D loss: 0.511283, acc.: 75.78%] [G loss: 1.230257, acc.: 21.09%]\n",
      "14 100 [D loss: 0.460524, acc.: 78.91%] [G loss: 1.382393, acc.: 15.62%]\n",
      "14 150 [D loss: 0.492929, acc.: 77.34%] [G loss: 1.421405, acc.: 17.19%]\n",
      "14 200 [D loss: 0.462472, acc.: 77.34%] [G loss: 1.319399, acc.: 19.53%]\n",
      "14 250 [D loss: 0.441646, acc.: 78.91%] [G loss: 1.312152, acc.: 20.31%]\n",
      "14 300 [D loss: 0.448522, acc.: 79.30%] [G loss: 1.522073, acc.: 10.16%]\n",
      "14 350 [D loss: 0.503361, acc.: 76.95%] [G loss: 1.533428, acc.: 13.28%]\n",
      "14 400 [D loss: 0.454644, acc.: 79.69%] [G loss: 1.421941, acc.: 13.28%]\n",
      "14 450 [D loss: 0.453688, acc.: 79.30%] [G loss: 1.395025, acc.: 17.19%]\n",
      "15 0 [D loss: 0.471287, acc.: 78.91%] [G loss: 1.219754, acc.: 17.19%]\n",
      "15 50 [D loss: 0.468385, acc.: 78.12%] [G loss: 1.349230, acc.: 16.41%]\n",
      "15 100 [D loss: 0.480689, acc.: 80.47%] [G loss: 1.401876, acc.: 13.28%]\n",
      "15 150 [D loss: 0.494852, acc.: 80.86%] [G loss: 1.347614, acc.: 10.94%]\n",
      "15 200 [D loss: 0.545519, acc.: 73.05%] [G loss: 1.177145, acc.: 21.09%]\n",
      "15 250 [D loss: 0.452050, acc.: 82.03%] [G loss: 1.355584, acc.: 12.50%]\n",
      "15 300 [D loss: 0.489568, acc.: 76.56%] [G loss: 1.297670, acc.: 18.75%]\n",
      "15 350 [D loss: 0.499135, acc.: 76.56%] [G loss: 1.327821, acc.: 15.62%]\n",
      "15 400 [D loss: 0.496011, acc.: 73.44%] [G loss: 1.374598, acc.: 15.62%]\n",
      "15 450 [D loss: 0.497632, acc.: 77.34%] [G loss: 1.269577, acc.: 14.06%]\n",
      "16 0 [D loss: 0.460984, acc.: 76.56%] [G loss: 1.345301, acc.: 15.62%]\n",
      "16 50 [D loss: 0.462441, acc.: 79.30%] [G loss: 1.342761, acc.: 13.28%]\n",
      "16 100 [D loss: 0.507535, acc.: 74.61%] [G loss: 1.313346, acc.: 16.41%]\n",
      "16 150 [D loss: 0.465249, acc.: 80.47%] [G loss: 1.357056, acc.: 14.06%]\n",
      "16 200 [D loss: 0.439471, acc.: 82.42%] [G loss: 1.297609, acc.: 17.19%]\n",
      "16 250 [D loss: 0.439436, acc.: 80.47%] [G loss: 1.344090, acc.: 13.28%]\n",
      "16 300 [D loss: 0.458297, acc.: 78.52%] [G loss: 1.268962, acc.: 19.53%]\n",
      "16 350 [D loss: 0.484629, acc.: 73.83%] [G loss: 1.329628, acc.: 16.41%]\n",
      "16 400 [D loss: 0.509786, acc.: 76.17%] [G loss: 1.272087, acc.: 13.28%]\n",
      "16 450 [D loss: 0.450550, acc.: 80.08%] [G loss: 1.431108, acc.: 12.50%]\n",
      "17 0 [D loss: 0.445748, acc.: 80.86%] [G loss: 1.377585, acc.: 12.50%]\n",
      "17 50 [D loss: 0.459881, acc.: 77.73%] [G loss: 1.371652, acc.: 10.94%]\n",
      "17 100 [D loss: 0.503833, acc.: 76.95%] [G loss: 1.268978, acc.: 17.19%]\n",
      "17 150 [D loss: 0.507455, acc.: 76.56%] [G loss: 1.264526, acc.: 14.06%]\n",
      "17 200 [D loss: 0.482316, acc.: 72.66%] [G loss: 1.227917, acc.: 21.88%]\n",
      "17 250 [D loss: 0.455738, acc.: 79.69%] [G loss: 1.348773, acc.: 13.28%]\n",
      "17 300 [D loss: 0.475188, acc.: 79.30%] [G loss: 1.306721, acc.: 17.19%]\n",
      "17 350 [D loss: 0.518868, acc.: 74.61%] [G loss: 1.357687, acc.: 13.28%]\n",
      "17 400 [D loss: 0.478943, acc.: 75.78%] [G loss: 1.210769, acc.: 19.53%]\n",
      "17 450 [D loss: 0.487811, acc.: 76.56%] [G loss: 1.277863, acc.: 17.19%]\n",
      "18 0 [D loss: 0.550919, acc.: 70.70%] [G loss: 1.367146, acc.: 16.41%]\n",
      "18 50 [D loss: 0.458667, acc.: 79.30%] [G loss: 1.381349, acc.: 16.41%]\n",
      "18 100 [D loss: 0.484181, acc.: 77.73%] [G loss: 1.301323, acc.: 16.41%]\n",
      "18 150 [D loss: 0.446358, acc.: 79.69%] [G loss: 1.328794, acc.: 14.84%]\n",
      "18 200 [D loss: 0.492227, acc.: 76.17%] [G loss: 1.252618, acc.: 19.53%]\n",
      "18 250 [D loss: 0.500401, acc.: 76.95%] [G loss: 1.230694, acc.: 21.09%]\n",
      "18 300 [D loss: 0.488590, acc.: 78.12%] [G loss: 1.366879, acc.: 10.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 350 [D loss: 0.513635, acc.: 73.05%] [G loss: 1.214139, acc.: 21.09%]\n",
      "18 400 [D loss: 0.488209, acc.: 78.12%] [G loss: 1.251646, acc.: 18.75%]\n",
      "18 450 [D loss: 0.484658, acc.: 77.34%] [G loss: 1.306176, acc.: 13.28%]\n",
      "19 0 [D loss: 0.495206, acc.: 76.17%] [G loss: 1.229553, acc.: 23.44%]\n",
      "19 50 [D loss: 0.509863, acc.: 76.95%] [G loss: 1.306238, acc.: 14.06%]\n",
      "19 100 [D loss: 0.524494, acc.: 76.17%] [G loss: 1.251524, acc.: 14.84%]\n",
      "19 150 [D loss: 0.496396, acc.: 77.73%] [G loss: 1.369751, acc.: 9.38%]\n",
      "19 200 [D loss: 0.465988, acc.: 76.95%] [G loss: 1.272873, acc.: 17.97%]\n",
      "19 250 [D loss: 0.555810, acc.: 71.88%] [G loss: 1.206947, acc.: 19.53%]\n",
      "19 300 [D loss: 0.499247, acc.: 76.56%] [G loss: 1.240422, acc.: 19.53%]\n",
      "19 350 [D loss: 0.521939, acc.: 76.95%] [G loss: 1.166323, acc.: 17.97%]\n",
      "19 400 [D loss: 0.492757, acc.: 75.78%] [G loss: 1.223638, acc.: 21.88%]\n",
      "19 450 [D loss: 0.454394, acc.: 79.30%] [G loss: 1.206277, acc.: 14.06%]\n",
      "20 0 [D loss: 0.491519, acc.: 77.73%] [G loss: 1.333728, acc.: 17.19%]\n",
      "20 50 [D loss: 0.502740, acc.: 73.83%] [G loss: 1.379122, acc.: 14.06%]\n",
      "20 100 [D loss: 0.477293, acc.: 78.12%] [G loss: 1.279818, acc.: 17.19%]\n",
      "20 150 [D loss: 0.488329, acc.: 76.95%] [G loss: 1.192364, acc.: 22.66%]\n",
      "20 200 [D loss: 0.480969, acc.: 78.12%] [G loss: 1.163859, acc.: 17.19%]\n",
      "20 250 [D loss: 0.496824, acc.: 78.12%] [G loss: 1.303956, acc.: 16.41%]\n",
      "20 300 [D loss: 0.497624, acc.: 74.61%] [G loss: 1.291086, acc.: 16.41%]\n",
      "20 350 [D loss: 0.544359, acc.: 71.88%] [G loss: 1.213696, acc.: 25.78%]\n",
      "20 400 [D loss: 0.502662, acc.: 76.17%] [G loss: 1.255441, acc.: 10.16%]\n",
      "20 450 [D loss: 0.517118, acc.: 71.48%] [G loss: 1.239703, acc.: 23.44%]\n",
      "21 0 [D loss: 0.465584, acc.: 79.30%] [G loss: 1.237857, acc.: 18.75%]\n",
      "21 50 [D loss: 0.464079, acc.: 81.64%] [G loss: 1.321737, acc.: 12.50%]\n",
      "21 100 [D loss: 0.549294, acc.: 73.44%] [G loss: 1.304832, acc.: 16.41%]\n",
      "21 150 [D loss: 0.516459, acc.: 74.61%] [G loss: 1.188441, acc.: 19.53%]\n",
      "21 200 [D loss: 0.436242, acc.: 82.03%] [G loss: 1.343357, acc.: 14.06%]\n",
      "21 250 [D loss: 0.494784, acc.: 75.78%] [G loss: 1.260833, acc.: 20.31%]\n",
      "21 300 [D loss: 0.491650, acc.: 78.12%] [G loss: 1.216377, acc.: 14.84%]\n",
      "21 350 [D loss: 0.497534, acc.: 78.12%] [G loss: 1.352062, acc.: 15.62%]\n",
      "21 400 [D loss: 0.519274, acc.: 73.05%] [G loss: 1.221370, acc.: 16.41%]\n",
      "21 450 [D loss: 0.515469, acc.: 75.39%] [G loss: 1.196086, acc.: 17.97%]\n",
      "22 0 [D loss: 0.469114, acc.: 80.47%] [G loss: 1.271789, acc.: 16.41%]\n",
      "22 50 [D loss: 0.493890, acc.: 79.69%] [G loss: 1.225668, acc.: 18.75%]\n",
      "22 100 [D loss: 0.504218, acc.: 75.00%] [G loss: 1.189199, acc.: 20.31%]\n",
      "22 150 [D loss: 0.495762, acc.: 77.34%] [G loss: 1.132122, acc.: 21.88%]\n",
      "22 200 [D loss: 0.517114, acc.: 73.83%] [G loss: 1.175138, acc.: 20.31%]\n",
      "22 250 [D loss: 0.479088, acc.: 78.52%] [G loss: 1.356238, acc.: 14.06%]\n",
      "22 300 [D loss: 0.499629, acc.: 75.00%] [G loss: 1.260746, acc.: 14.84%]\n",
      "22 350 [D loss: 0.474435, acc.: 77.73%] [G loss: 1.244894, acc.: 17.19%]\n",
      "22 400 [D loss: 0.537001, acc.: 71.48%] [G loss: 1.182792, acc.: 24.22%]\n",
      "22 450 [D loss: 0.461733, acc.: 78.12%] [G loss: 1.216944, acc.: 17.97%]\n",
      "23 0 [D loss: 0.509005, acc.: 76.56%] [G loss: 1.181231, acc.: 23.44%]\n",
      "23 50 [D loss: 0.485397, acc.: 78.52%] [G loss: 1.207361, acc.: 18.75%]\n",
      "23 100 [D loss: 0.457986, acc.: 78.12%] [G loss: 1.336654, acc.: 15.62%]\n",
      "23 150 [D loss: 0.521721, acc.: 75.39%] [G loss: 1.270451, acc.: 19.53%]\n",
      "23 200 [D loss: 0.460397, acc.: 80.08%] [G loss: 1.251685, acc.: 14.84%]\n",
      "23 250 [D loss: 0.462362, acc.: 79.69%] [G loss: 1.196571, acc.: 19.53%]\n",
      "23 300 [D loss: 0.542100, acc.: 74.22%] [G loss: 1.165384, acc.: 17.19%]\n",
      "23 350 [D loss: 0.456596, acc.: 79.30%] [G loss: 1.211655, acc.: 19.53%]\n",
      "23 400 [D loss: 0.465978, acc.: 78.52%] [G loss: 1.325514, acc.: 13.28%]\n",
      "23 450 [D loss: 0.474972, acc.: 79.30%] [G loss: 1.252788, acc.: 12.50%]\n",
      "24 0 [D loss: 0.473133, acc.: 78.91%] [G loss: 1.293993, acc.: 15.62%]\n",
      "24 50 [D loss: 0.547824, acc.: 73.83%] [G loss: 1.222975, acc.: 17.97%]\n",
      "24 100 [D loss: 0.484507, acc.: 78.91%] [G loss: 1.198597, acc.: 24.22%]\n",
      "24 150 [D loss: 0.521548, acc.: 73.05%] [G loss: 1.150836, acc.: 17.19%]\n",
      "24 200 [D loss: 0.495593, acc.: 78.12%] [G loss: 1.121727, acc.: 21.09%]\n",
      "24 250 [D loss: 0.464817, acc.: 80.47%] [G loss: 1.261740, acc.: 12.50%]\n",
      "24 300 [D loss: 0.463410, acc.: 79.69%] [G loss: 1.301245, acc.: 13.28%]\n",
      "24 350 [D loss: 0.465628, acc.: 78.12%] [G loss: 1.336335, acc.: 11.72%]\n",
      "24 400 [D loss: 0.477713, acc.: 78.12%] [G loss: 1.310817, acc.: 10.94%]\n",
      "24 450 [D loss: 0.453330, acc.: 79.69%] [G loss: 1.300607, acc.: 14.06%]\n",
      "25 0 [D loss: 0.479281, acc.: 76.95%] [G loss: 1.280545, acc.: 12.50%]\n",
      "25 50 [D loss: 0.505596, acc.: 76.17%] [G loss: 1.195935, acc.: 19.53%]\n",
      "25 100 [D loss: 0.510287, acc.: 74.61%] [G loss: 1.279570, acc.: 18.75%]\n",
      "25 150 [D loss: 0.535295, acc.: 75.00%] [G loss: 1.210180, acc.: 17.97%]\n",
      "25 200 [D loss: 0.512407, acc.: 76.56%] [G loss: 1.192406, acc.: 21.88%]\n",
      "25 250 [D loss: 0.490309, acc.: 75.39%] [G loss: 1.158536, acc.: 22.66%]\n",
      "25 300 [D loss: 0.476083, acc.: 80.86%] [G loss: 1.165874, acc.: 22.66%]\n",
      "25 350 [D loss: 0.510151, acc.: 76.95%] [G loss: 1.198088, acc.: 17.97%]\n",
      "25 400 [D loss: 0.500028, acc.: 75.78%] [G loss: 1.229265, acc.: 19.53%]\n",
      "25 450 [D loss: 0.509665, acc.: 77.73%] [G loss: 1.139066, acc.: 19.53%]\n",
      "26 0 [D loss: 0.470460, acc.: 80.08%] [G loss: 1.291390, acc.: 13.28%]\n",
      "26 50 [D loss: 0.499233, acc.: 75.78%] [G loss: 1.173587, acc.: 18.75%]\n",
      "26 100 [D loss: 0.484619, acc.: 78.12%] [G loss: 1.188839, acc.: 17.97%]\n",
      "26 150 [D loss: 0.494072, acc.: 75.00%] [G loss: 1.247726, acc.: 14.84%]\n",
      "26 200 [D loss: 0.536412, acc.: 75.00%] [G loss: 1.175503, acc.: 20.31%]\n",
      "26 250 [D loss: 0.510386, acc.: 75.39%] [G loss: 1.220799, acc.: 17.97%]\n",
      "26 300 [D loss: 0.498353, acc.: 78.52%] [G loss: 1.158930, acc.: 17.97%]\n",
      "26 350 [D loss: 0.495603, acc.: 76.17%] [G loss: 1.216715, acc.: 17.97%]\n",
      "26 400 [D loss: 0.525884, acc.: 71.88%] [G loss: 1.207472, acc.: 21.09%]\n",
      "26 450 [D loss: 0.504637, acc.: 73.44%] [G loss: 1.241266, acc.: 14.06%]\n",
      "27 0 [D loss: 0.490445, acc.: 78.91%] [G loss: 1.157422, acc.: 19.53%]\n",
      "27 50 [D loss: 0.495085, acc.: 75.00%] [G loss: 1.233502, acc.: 17.19%]\n",
      "27 100 [D loss: 0.520334, acc.: 74.61%] [G loss: 1.153996, acc.: 20.31%]\n",
      "27 150 [D loss: 0.508025, acc.: 73.44%] [G loss: 1.188121, acc.: 17.97%]\n",
      "27 200 [D loss: 0.514991, acc.: 72.27%] [G loss: 1.172525, acc.: 18.75%]\n",
      "27 250 [D loss: 0.462332, acc.: 80.47%] [G loss: 1.296590, acc.: 10.94%]\n",
      "27 300 [D loss: 0.501947, acc.: 75.00%] [G loss: 1.175843, acc.: 18.75%]\n",
      "27 350 [D loss: 0.477821, acc.: 76.95%] [G loss: 1.241351, acc.: 16.41%]\n",
      "27 400 [D loss: 0.486024, acc.: 78.91%] [G loss: 1.181154, acc.: 17.19%]\n",
      "27 450 [D loss: 0.500214, acc.: 77.73%] [G loss: 1.148567, acc.: 21.88%]\n",
      "28 0 [D loss: 0.537247, acc.: 74.61%] [G loss: 1.152788, acc.: 21.09%]\n",
      "28 50 [D loss: 0.513929, acc.: 77.34%] [G loss: 1.210565, acc.: 14.84%]\n",
      "28 100 [D loss: 0.527925, acc.: 74.61%] [G loss: 1.207093, acc.: 14.06%]\n",
      "28 150 [D loss: 0.456562, acc.: 83.20%] [G loss: 1.228904, acc.: 19.53%]\n",
      "28 200 [D loss: 0.509894, acc.: 76.17%] [G loss: 1.148690, acc.: 17.97%]\n",
      "28 250 [D loss: 0.505252, acc.: 75.39%] [G loss: 1.249684, acc.: 16.41%]\n",
      "28 300 [D loss: 0.518227, acc.: 76.17%] [G loss: 1.224608, acc.: 17.97%]\n",
      "28 350 [D loss: 0.469764, acc.: 77.73%] [G loss: 1.271579, acc.: 14.84%]\n",
      "28 400 [D loss: 0.518268, acc.: 75.39%] [G loss: 1.121431, acc.: 23.44%]\n",
      "28 450 [D loss: 0.531076, acc.: 71.09%] [G loss: 1.140487, acc.: 21.09%]\n",
      "29 0 [D loss: 0.522786, acc.: 74.61%] [G loss: 1.266448, acc.: 17.19%]\n",
      "29 50 [D loss: 0.533825, acc.: 74.22%] [G loss: 1.172305, acc.: 20.31%]\n",
      "29 100 [D loss: 0.511546, acc.: 75.39%] [G loss: 1.151198, acc.: 21.09%]\n",
      "29 150 [D loss: 0.515002, acc.: 76.56%] [G loss: 1.264913, acc.: 17.19%]\n",
      "29 200 [D loss: 0.523633, acc.: 75.00%] [G loss: 1.157699, acc.: 18.75%]\n",
      "29 250 [D loss: 0.509510, acc.: 74.61%] [G loss: 1.199352, acc.: 17.97%]\n",
      "29 300 [D loss: 0.510484, acc.: 74.61%] [G loss: 1.203541, acc.: 18.75%]\n",
      "29 350 [D loss: 0.544721, acc.: 70.70%] [G loss: 1.212535, acc.: 22.66%]\n",
      "29 400 [D loss: 0.505414, acc.: 77.73%] [G loss: 1.161933, acc.: 25.00%]\n",
      "29 450 [D loss: 0.490535, acc.: 76.17%] [G loss: 1.202984, acc.: 19.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0 [D loss: 0.492449, acc.: 74.61%] [G loss: 1.204600, acc.: 21.09%]\n",
      "30 50 [D loss: 0.487481, acc.: 76.56%] [G loss: 1.173996, acc.: 21.88%]\n",
      "30 100 [D loss: 0.529163, acc.: 76.17%] [G loss: 1.178397, acc.: 15.62%]\n",
      "30 150 [D loss: 0.528704, acc.: 75.39%] [G loss: 1.186449, acc.: 19.53%]\n",
      "30 200 [D loss: 0.530522, acc.: 71.88%] [G loss: 1.115368, acc.: 24.22%]\n",
      "30 250 [D loss: 0.505788, acc.: 76.95%] [G loss: 1.255973, acc.: 10.94%]\n",
      "30 300 [D loss: 0.554526, acc.: 71.48%] [G loss: 1.049640, acc.: 28.12%]\n",
      "30 350 [D loss: 0.513671, acc.: 76.95%] [G loss: 1.142732, acc.: 20.31%]\n",
      "30 400 [D loss: 0.476049, acc.: 77.34%] [G loss: 1.224875, acc.: 15.62%]\n",
      "30 450 [D loss: 0.515460, acc.: 74.22%] [G loss: 1.246697, acc.: 11.72%]\n",
      "31 0 [D loss: 0.532045, acc.: 73.05%] [G loss: 1.195916, acc.: 14.84%]\n",
      "31 50 [D loss: 0.503788, acc.: 75.78%] [G loss: 1.191438, acc.: 21.88%]\n",
      "31 100 [D loss: 0.500402, acc.: 74.61%] [G loss: 1.138549, acc.: 19.53%]\n",
      "31 150 [D loss: 0.462133, acc.: 79.30%] [G loss: 1.301240, acc.: 11.72%]\n",
      "31 200 [D loss: 0.500674, acc.: 78.91%] [G loss: 1.157902, acc.: 19.53%]\n",
      "31 250 [D loss: 0.545898, acc.: 70.70%] [G loss: 1.086719, acc.: 25.00%]\n",
      "31 300 [D loss: 0.511846, acc.: 76.95%] [G loss: 1.264030, acc.: 11.72%]\n",
      "31 350 [D loss: 0.469964, acc.: 78.91%] [G loss: 1.306719, acc.: 13.28%]\n",
      "31 400 [D loss: 0.493237, acc.: 74.61%] [G loss: 1.272504, acc.: 14.84%]\n",
      "31 450 [D loss: 0.512305, acc.: 77.34%] [G loss: 1.135861, acc.: 25.00%]\n",
      "32 0 [D loss: 0.580723, acc.: 71.09%] [G loss: 1.188717, acc.: 13.28%]\n",
      "32 50 [D loss: 0.549927, acc.: 75.00%] [G loss: 1.137207, acc.: 17.97%]\n",
      "32 100 [D loss: 0.537941, acc.: 71.88%] [G loss: 1.186825, acc.: 21.09%]\n",
      "32 150 [D loss: 0.498444, acc.: 76.95%] [G loss: 1.181456, acc.: 16.41%]\n",
      "32 200 [D loss: 0.485511, acc.: 75.00%] [G loss: 1.181496, acc.: 18.75%]\n",
      "32 250 [D loss: 0.526211, acc.: 76.17%] [G loss: 1.188653, acc.: 18.75%]\n",
      "32 300 [D loss: 0.509860, acc.: 75.00%] [G loss: 1.234781, acc.: 18.75%]\n",
      "32 350 [D loss: 0.520079, acc.: 76.95%] [G loss: 1.171938, acc.: 21.88%]\n",
      "32 400 [D loss: 0.498888, acc.: 77.73%] [G loss: 1.203585, acc.: 15.62%]\n",
      "32 450 [D loss: 0.518599, acc.: 76.95%] [G loss: 1.138732, acc.: 21.88%]\n",
      "33 0 [D loss: 0.510368, acc.: 75.39%] [G loss: 1.169600, acc.: 15.62%]\n",
      "33 50 [D loss: 0.521321, acc.: 74.61%] [G loss: 1.132470, acc.: 15.62%]\n",
      "33 100 [D loss: 0.527340, acc.: 75.78%] [G loss: 1.197694, acc.: 17.19%]\n",
      "33 150 [D loss: 0.528233, acc.: 74.61%] [G loss: 1.125303, acc.: 25.00%]\n",
      "33 200 [D loss: 0.505535, acc.: 76.56%] [G loss: 1.204218, acc.: 14.84%]\n",
      "33 250 [D loss: 0.541542, acc.: 71.48%] [G loss: 1.081752, acc.: 20.31%]\n",
      "33 300 [D loss: 0.511156, acc.: 75.78%] [G loss: 1.129314, acc.: 25.00%]\n",
      "33 350 [D loss: 0.534156, acc.: 73.05%] [G loss: 1.174059, acc.: 19.53%]\n",
      "33 400 [D loss: 0.473678, acc.: 80.08%] [G loss: 1.179754, acc.: 19.53%]\n",
      "33 450 [D loss: 0.530705, acc.: 74.22%] [G loss: 1.112947, acc.: 20.31%]\n",
      "34 0 [D loss: 0.534867, acc.: 74.61%] [G loss: 1.089109, acc.: 20.31%]\n",
      "34 50 [D loss: 0.488934, acc.: 77.34%] [G loss: 1.143376, acc.: 17.97%]\n",
      "34 100 [D loss: 0.528581, acc.: 72.27%] [G loss: 1.152044, acc.: 24.22%]\n",
      "34 150 [D loss: 0.520624, acc.: 74.22%] [G loss: 1.211118, acc.: 17.97%]\n",
      "34 200 [D loss: 0.451373, acc.: 81.25%] [G loss: 1.184621, acc.: 18.75%]\n",
      "34 250 [D loss: 0.544831, acc.: 70.31%] [G loss: 1.090392, acc.: 21.09%]\n",
      "34 300 [D loss: 0.476086, acc.: 80.47%] [G loss: 1.182940, acc.: 16.41%]\n",
      "34 350 [D loss: 0.480978, acc.: 76.56%] [G loss: 1.170540, acc.: 19.53%]\n",
      "34 400 [D loss: 0.541822, acc.: 73.05%] [G loss: 1.213554, acc.: 21.88%]\n",
      "34 450 [D loss: 0.522871, acc.: 75.00%] [G loss: 1.199725, acc.: 14.84%]\n",
      "35 0 [D loss: 0.525612, acc.: 72.66%] [G loss: 1.175005, acc.: 21.09%]\n",
      "35 50 [D loss: 0.524088, acc.: 71.48%] [G loss: 1.085330, acc.: 25.78%]\n",
      "35 100 [D loss: 0.542790, acc.: 71.88%] [G loss: 1.128344, acc.: 24.22%]\n",
      "35 150 [D loss: 0.511985, acc.: 74.22%] [G loss: 1.189538, acc.: 15.62%]\n",
      "35 200 [D loss: 0.527092, acc.: 74.22%] [G loss: 1.205282, acc.: 15.62%]\n",
      "35 250 [D loss: 0.504186, acc.: 78.52%] [G loss: 1.193635, acc.: 13.28%]\n",
      "35 300 [D loss: 0.489954, acc.: 75.00%] [G loss: 1.162418, acc.: 18.75%]\n",
      "35 350 [D loss: 0.489542, acc.: 76.17%] [G loss: 1.212873, acc.: 15.62%]\n",
      "35 400 [D loss: 0.480737, acc.: 79.69%] [G loss: 1.226168, acc.: 13.28%]\n",
      "35 450 [D loss: 0.484730, acc.: 73.44%] [G loss: 1.258676, acc.: 17.19%]\n",
      "36 0 [D loss: 0.443000, acc.: 83.59%] [G loss: 1.222656, acc.: 15.62%]\n",
      "36 50 [D loss: 0.517318, acc.: 74.61%] [G loss: 1.173927, acc.: 18.75%]\n",
      "36 100 [D loss: 0.485899, acc.: 78.52%] [G loss: 1.175460, acc.: 16.41%]\n",
      "36 150 [D loss: 0.469483, acc.: 78.52%] [G loss: 1.257596, acc.: 14.84%]\n",
      "36 200 [D loss: 0.485490, acc.: 74.61%] [G loss: 1.156032, acc.: 23.44%]\n",
      "36 250 [D loss: 0.497542, acc.: 75.39%] [G loss: 1.271197, acc.: 14.06%]\n",
      "36 300 [D loss: 0.534855, acc.: 73.44%] [G loss: 1.105587, acc.: 22.66%]\n",
      "36 350 [D loss: 0.488896, acc.: 76.17%] [G loss: 1.177669, acc.: 20.31%]\n",
      "36 400 [D loss: 0.509760, acc.: 77.34%] [G loss: 1.190382, acc.: 10.16%]\n",
      "36 450 [D loss: 0.515083, acc.: 75.39%] [G loss: 1.119726, acc.: 21.09%]\n",
      "37 0 [D loss: 0.498380, acc.: 76.95%] [G loss: 1.147817, acc.: 22.66%]\n",
      "37 50 [D loss: 0.497833, acc.: 75.78%] [G loss: 1.296437, acc.: 15.62%]\n",
      "37 100 [D loss: 0.505879, acc.: 77.73%] [G loss: 1.190031, acc.: 18.75%]\n",
      "37 150 [D loss: 0.520725, acc.: 75.78%] [G loss: 1.255792, acc.: 15.62%]\n",
      "37 200 [D loss: 0.530434, acc.: 74.22%] [G loss: 1.177835, acc.: 14.84%]\n",
      "37 250 [D loss: 0.524640, acc.: 76.56%] [G loss: 1.169633, acc.: 17.97%]\n",
      "37 300 [D loss: 0.539500, acc.: 73.05%] [G loss: 1.107394, acc.: 21.88%]\n",
      "37 350 [D loss: 0.545754, acc.: 74.22%] [G loss: 1.090364, acc.: 19.53%]\n",
      "37 400 [D loss: 0.554352, acc.: 73.83%] [G loss: 1.164289, acc.: 20.31%]\n",
      "37 450 [D loss: 0.538005, acc.: 71.88%] [G loss: 1.179959, acc.: 23.44%]\n",
      "38 0 [D loss: 0.519183, acc.: 73.05%] [G loss: 1.159521, acc.: 16.41%]\n",
      "38 50 [D loss: 0.505933, acc.: 73.83%] [G loss: 1.142215, acc.: 15.62%]\n",
      "38 100 [D loss: 0.533229, acc.: 74.22%] [G loss: 1.205411, acc.: 14.84%]\n",
      "38 150 [D loss: 0.468852, acc.: 78.91%] [G loss: 1.202101, acc.: 14.06%]\n",
      "38 200 [D loss: 0.510990, acc.: 75.39%] [G loss: 1.117579, acc.: 25.78%]\n",
      "38 250 [D loss: 0.538387, acc.: 72.66%] [G loss: 1.174371, acc.: 21.88%]\n",
      "38 300 [D loss: 0.551169, acc.: 70.31%] [G loss: 1.066586, acc.: 22.66%]\n",
      "38 350 [D loss: 0.523687, acc.: 72.66%] [G loss: 1.110199, acc.: 23.44%]\n",
      "38 400 [D loss: 0.557627, acc.: 73.44%] [G loss: 1.102133, acc.: 22.66%]\n",
      "38 450 [D loss: 0.553097, acc.: 71.09%] [G loss: 1.123390, acc.: 19.53%]\n",
      "39 0 [D loss: 0.568323, acc.: 71.48%] [G loss: 1.036271, acc.: 27.34%]\n",
      "39 50 [D loss: 0.541334, acc.: 70.70%] [G loss: 1.071443, acc.: 22.66%]\n",
      "39 100 [D loss: 0.474265, acc.: 78.52%] [G loss: 1.177354, acc.: 19.53%]\n",
      "39 150 [D loss: 0.553014, acc.: 70.31%] [G loss: 1.155571, acc.: 22.66%]\n",
      "39 200 [D loss: 0.526392, acc.: 71.09%] [G loss: 1.134630, acc.: 23.44%]\n",
      "39 250 [D loss: 0.510934, acc.: 70.70%] [G loss: 1.168865, acc.: 17.19%]\n",
      "39 300 [D loss: 0.523521, acc.: 75.39%] [G loss: 1.185520, acc.: 22.66%]\n",
      "39 350 [D loss: 0.554355, acc.: 72.66%] [G loss: 1.111769, acc.: 25.78%]\n",
      "39 400 [D loss: 0.507931, acc.: 75.39%] [G loss: 1.123289, acc.: 20.31%]\n",
      "39 450 [D loss: 0.537367, acc.: 75.78%] [G loss: 1.132815, acc.: 20.31%]\n",
      "40 0 [D loss: 0.503537, acc.: 76.95%] [G loss: 1.164765, acc.: 16.41%]\n",
      "40 50 [D loss: 0.463556, acc.: 75.00%] [G loss: 1.203871, acc.: 21.09%]\n",
      "40 100 [D loss: 0.549545, acc.: 74.61%] [G loss: 1.106217, acc.: 24.22%]\n",
      "40 150 [D loss: 0.504207, acc.: 75.78%] [G loss: 1.133712, acc.: 19.53%]\n",
      "40 200 [D loss: 0.519619, acc.: 74.22%] [G loss: 1.143024, acc.: 17.97%]\n",
      "40 250 [D loss: 0.521756, acc.: 74.61%] [G loss: 1.080905, acc.: 23.44%]\n",
      "40 300 [D loss: 0.504541, acc.: 78.52%] [G loss: 1.199389, acc.: 20.31%]\n",
      "40 350 [D loss: 0.510279, acc.: 73.83%] [G loss: 1.128964, acc.: 17.97%]\n",
      "40 400 [D loss: 0.508415, acc.: 73.44%] [G loss: 1.228774, acc.: 18.75%]\n",
      "40 450 [D loss: 0.526212, acc.: 72.66%] [G loss: 1.122799, acc.: 26.56%]\n",
      "41 0 [D loss: 0.495654, acc.: 80.47%] [G loss: 1.156319, acc.: 17.97%]\n",
      "41 50 [D loss: 0.584711, acc.: 67.19%] [G loss: 1.114083, acc.: 19.53%]\n",
      "41 100 [D loss: 0.534239, acc.: 76.95%] [G loss: 1.104490, acc.: 16.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 150 [D loss: 0.537245, acc.: 73.05%] [G loss: 1.161621, acc.: 17.97%]\n",
      "41 200 [D loss: 0.500524, acc.: 75.00%] [G loss: 1.158519, acc.: 17.97%]\n",
      "41 250 [D loss: 0.536505, acc.: 73.05%] [G loss: 1.123056, acc.: 23.44%]\n",
      "41 300 [D loss: 0.482826, acc.: 75.78%] [G loss: 1.193088, acc.: 20.31%]\n",
      "41 350 [D loss: 0.526603, acc.: 78.91%] [G loss: 1.086971, acc.: 24.22%]\n",
      "41 400 [D loss: 0.504180, acc.: 76.56%] [G loss: 1.152435, acc.: 15.62%]\n",
      "41 450 [D loss: 0.552327, acc.: 73.44%] [G loss: 1.106067, acc.: 26.56%]\n",
      "42 0 [D loss: 0.511043, acc.: 75.39%] [G loss: 1.089278, acc.: 18.75%]\n",
      "42 50 [D loss: 0.534058, acc.: 74.61%] [G loss: 1.123866, acc.: 21.09%]\n",
      "42 100 [D loss: 0.528888, acc.: 71.88%] [G loss: 1.130122, acc.: 21.88%]\n",
      "42 150 [D loss: 0.498924, acc.: 77.34%] [G loss: 1.204607, acc.: 14.84%]\n",
      "42 200 [D loss: 0.484959, acc.: 78.91%] [G loss: 1.197028, acc.: 17.19%]\n",
      "42 250 [D loss: 0.528579, acc.: 73.44%] [G loss: 1.138817, acc.: 17.97%]\n",
      "42 300 [D loss: 0.535607, acc.: 73.44%] [G loss: 1.174587, acc.: 18.75%]\n",
      "42 350 [D loss: 0.485567, acc.: 77.73%] [G loss: 1.159034, acc.: 15.62%]\n",
      "42 400 [D loss: 0.539428, acc.: 73.44%] [G loss: 1.038768, acc.: 24.22%]\n",
      "42 450 [D loss: 0.550074, acc.: 71.88%] [G loss: 1.156589, acc.: 20.31%]\n",
      "43 0 [D loss: 0.567127, acc.: 69.14%] [G loss: 1.158877, acc.: 18.75%]\n",
      "43 50 [D loss: 0.524001, acc.: 76.95%] [G loss: 1.124162, acc.: 21.88%]\n",
      "43 100 [D loss: 0.567596, acc.: 69.53%] [G loss: 1.103436, acc.: 23.44%]\n",
      "43 150 [D loss: 0.531120, acc.: 72.66%] [G loss: 1.092118, acc.: 22.66%]\n",
      "43 200 [D loss: 0.536940, acc.: 75.78%] [G loss: 1.072122, acc.: 21.88%]\n",
      "43 250 [D loss: 0.501796, acc.: 76.17%] [G loss: 1.198430, acc.: 14.84%]\n",
      "43 300 [D loss: 0.539205, acc.: 74.61%] [G loss: 1.143247, acc.: 17.97%]\n",
      "43 350 [D loss: 0.524194, acc.: 76.17%] [G loss: 1.162344, acc.: 18.75%]\n",
      "43 400 [D loss: 0.520839, acc.: 74.22%] [G loss: 1.166858, acc.: 19.53%]\n",
      "43 450 [D loss: 0.536951, acc.: 72.27%] [G loss: 1.117368, acc.: 27.34%]\n",
      "44 0 [D loss: 0.561680, acc.: 73.44%] [G loss: 1.121546, acc.: 18.75%]\n",
      "44 50 [D loss: 0.494940, acc.: 78.52%] [G loss: 1.161640, acc.: 19.53%]\n",
      "44 100 [D loss: 0.560399, acc.: 73.83%] [G loss: 1.159910, acc.: 21.09%]\n",
      "44 150 [D loss: 0.504327, acc.: 75.00%] [G loss: 1.168929, acc.: 17.19%]\n",
      "44 200 [D loss: 0.526659, acc.: 77.73%] [G loss: 1.156896, acc.: 17.97%]\n",
      "44 250 [D loss: 0.523978, acc.: 76.56%] [G loss: 1.122346, acc.: 17.97%]\n",
      "44 300 [D loss: 0.533517, acc.: 73.05%] [G loss: 1.159870, acc.: 18.75%]\n",
      "44 350 [D loss: 0.511283, acc.: 77.34%] [G loss: 1.116982, acc.: 18.75%]\n",
      "44 400 [D loss: 0.556993, acc.: 73.05%] [G loss: 1.034791, acc.: 28.91%]\n",
      "44 450 [D loss: 0.515369, acc.: 75.39%] [G loss: 1.107775, acc.: 17.97%]\n",
      "45 0 [D loss: 0.554571, acc.: 71.48%] [G loss: 1.103047, acc.: 23.44%]\n",
      "45 50 [D loss: 0.511155, acc.: 75.78%] [G loss: 1.118754, acc.: 17.97%]\n",
      "45 100 [D loss: 0.519107, acc.: 73.83%] [G loss: 1.165105, acc.: 21.09%]\n",
      "45 150 [D loss: 0.538452, acc.: 73.44%] [G loss: 1.198942, acc.: 14.84%]\n",
      "45 200 [D loss: 0.541973, acc.: 71.48%] [G loss: 1.139895, acc.: 21.09%]\n",
      "45 250 [D loss: 0.535387, acc.: 73.44%] [G loss: 1.090915, acc.: 24.22%]\n",
      "45 300 [D loss: 0.484152, acc.: 74.61%] [G loss: 1.163892, acc.: 21.88%]\n",
      "45 350 [D loss: 0.557044, acc.: 73.44%] [G loss: 1.101029, acc.: 19.53%]\n",
      "45 400 [D loss: 0.529608, acc.: 73.44%] [G loss: 1.084748, acc.: 21.09%]\n",
      "45 450 [D loss: 0.486800, acc.: 78.12%] [G loss: 1.181440, acc.: 16.41%]\n",
      "46 0 [D loss: 0.483465, acc.: 79.30%] [G loss: 1.212730, acc.: 14.84%]\n",
      "46 50 [D loss: 0.507844, acc.: 75.00%] [G loss: 1.159341, acc.: 21.88%]\n",
      "46 100 [D loss: 0.490226, acc.: 79.30%] [G loss: 1.244117, acc.: 16.41%]\n",
      "46 150 [D loss: 0.456292, acc.: 83.59%] [G loss: 1.278379, acc.: 15.62%]\n",
      "46 200 [D loss: 0.529532, acc.: 75.00%] [G loss: 1.111645, acc.: 25.00%]\n",
      "46 250 [D loss: 0.529672, acc.: 75.39%] [G loss: 1.132108, acc.: 14.06%]\n",
      "46 300 [D loss: 0.544551, acc.: 69.92%] [G loss: 1.129519, acc.: 26.56%]\n",
      "46 350 [D loss: 0.527998, acc.: 75.39%] [G loss: 1.128394, acc.: 17.19%]\n",
      "46 400 [D loss: 0.540602, acc.: 71.09%] [G loss: 1.068914, acc.: 24.22%]\n",
      "46 450 [D loss: 0.526005, acc.: 73.83%] [G loss: 1.144695, acc.: 21.88%]\n",
      "47 0 [D loss: 0.546386, acc.: 74.61%] [G loss: 1.223969, acc.: 16.41%]\n",
      "47 50 [D loss: 0.533487, acc.: 73.44%] [G loss: 1.138078, acc.: 21.88%]\n",
      "47 100 [D loss: 0.522780, acc.: 75.00%] [G loss: 1.153616, acc.: 17.97%]\n",
      "47 150 [D loss: 0.518242, acc.: 76.56%] [G loss: 1.202808, acc.: 15.62%]\n",
      "47 200 [D loss: 0.526073, acc.: 75.39%] [G loss: 1.179878, acc.: 17.19%]\n",
      "47 250 [D loss: 0.516028, acc.: 74.22%] [G loss: 1.161647, acc.: 22.66%]\n",
      "47 300 [D loss: 0.482928, acc.: 80.47%] [G loss: 1.170693, acc.: 13.28%]\n",
      "47 350 [D loss: 0.531091, acc.: 72.27%] [G loss: 1.058303, acc.: 28.91%]\n",
      "47 400 [D loss: 0.525419, acc.: 76.95%] [G loss: 1.134646, acc.: 21.09%]\n",
      "47 450 [D loss: 0.554758, acc.: 73.83%] [G loss: 1.103560, acc.: 20.31%]\n",
      "48 0 [D loss: 0.544222, acc.: 74.22%] [G loss: 1.123850, acc.: 21.88%]\n",
      "48 50 [D loss: 0.542313, acc.: 73.05%] [G loss: 1.038963, acc.: 31.25%]\n",
      "48 100 [D loss: 0.565566, acc.: 68.36%] [G loss: 1.082829, acc.: 27.34%]\n",
      "48 150 [D loss: 0.538032, acc.: 71.88%] [G loss: 1.126574, acc.: 20.31%]\n",
      "48 200 [D loss: 0.482391, acc.: 79.30%] [G loss: 1.206061, acc.: 17.19%]\n",
      "48 250 [D loss: 0.529117, acc.: 76.17%] [G loss: 1.126083, acc.: 24.22%]\n",
      "48 300 [D loss: 0.535226, acc.: 70.70%] [G loss: 1.059116, acc.: 22.66%]\n",
      "48 350 [D loss: 0.495648, acc.: 73.83%] [G loss: 1.207200, acc.: 17.97%]\n",
      "48 400 [D loss: 0.586587, acc.: 67.19%] [G loss: 1.061972, acc.: 18.75%]\n",
      "48 450 [D loss: 0.479979, acc.: 78.91%] [G loss: 1.183751, acc.: 20.31%]\n",
      "49 0 [D loss: 0.543693, acc.: 71.88%] [G loss: 1.093796, acc.: 26.56%]\n",
      "49 50 [D loss: 0.527960, acc.: 74.22%] [G loss: 1.087780, acc.: 21.88%]\n",
      "49 100 [D loss: 0.496322, acc.: 77.34%] [G loss: 1.193268, acc.: 11.72%]\n",
      "49 150 [D loss: 0.559686, acc.: 72.66%] [G loss: 1.102981, acc.: 21.88%]\n",
      "49 200 [D loss: 0.532505, acc.: 74.61%] [G loss: 1.222209, acc.: 19.53%]\n",
      "49 250 [D loss: 0.509718, acc.: 76.17%] [G loss: 1.123955, acc.: 20.31%]\n",
      "49 300 [D loss: 0.484594, acc.: 80.86%] [G loss: 1.152312, acc.: 18.75%]\n",
      "49 350 [D loss: 0.517887, acc.: 74.61%] [G loss: 1.218193, acc.: 11.72%]\n",
      "49 400 [D loss: 0.489353, acc.: 75.00%] [G loss: 1.097657, acc.: 21.88%]\n",
      "49 450 [D loss: 0.514677, acc.: 73.44%] [G loss: 1.163632, acc.: 21.88%]\n",
      "50 0 [D loss: 0.524257, acc.: 76.95%] [G loss: 1.194312, acc.: 17.97%]\n",
      "50 50 [D loss: 0.526596, acc.: 73.05%] [G loss: 1.148274, acc.: 21.09%]\n",
      "50 100 [D loss: 0.555780, acc.: 73.44%] [G loss: 1.128964, acc.: 18.75%]\n",
      "50 150 [D loss: 0.486216, acc.: 80.08%] [G loss: 1.192644, acc.: 17.97%]\n",
      "50 200 [D loss: 0.527267, acc.: 76.95%] [G loss: 1.158604, acc.: 16.41%]\n",
      "50 250 [D loss: 0.542626, acc.: 75.00%] [G loss: 1.048126, acc.: 25.00%]\n",
      "50 300 [D loss: 0.518182, acc.: 74.22%] [G loss: 1.119832, acc.: 21.09%]\n",
      "50 350 [D loss: 0.514944, acc.: 76.56%] [G loss: 1.157975, acc.: 15.62%]\n",
      "50 400 [D loss: 0.491315, acc.: 75.39%] [G loss: 1.156327, acc.: 18.75%]\n",
      "50 450 [D loss: 0.529111, acc.: 71.88%] [G loss: 1.169854, acc.: 21.09%]\n",
      "51 0 [D loss: 0.523164, acc.: 73.83%] [G loss: 1.116242, acc.: 27.34%]\n",
      "51 50 [D loss: 0.524771, acc.: 73.44%] [G loss: 1.128731, acc.: 22.66%]\n",
      "51 100 [D loss: 0.549214, acc.: 72.27%] [G loss: 1.169830, acc.: 21.09%]\n",
      "51 150 [D loss: 0.540223, acc.: 73.05%] [G loss: 1.137610, acc.: 21.88%]\n",
      "51 200 [D loss: 0.494464, acc.: 78.12%] [G loss: 1.202597, acc.: 16.41%]\n",
      "51 250 [D loss: 0.560584, acc.: 70.70%] [G loss: 1.073580, acc.: 23.44%]\n",
      "51 300 [D loss: 0.492917, acc.: 77.34%] [G loss: 1.172238, acc.: 17.19%]\n",
      "51 350 [D loss: 0.538872, acc.: 73.44%] [G loss: 1.125852, acc.: 20.31%]\n",
      "51 400 [D loss: 0.564530, acc.: 71.09%] [G loss: 1.198909, acc.: 22.66%]\n",
      "51 450 [D loss: 0.540119, acc.: 74.22%] [G loss: 1.173513, acc.: 17.19%]\n",
      "52 0 [D loss: 0.549596, acc.: 70.70%] [G loss: 1.135016, acc.: 21.88%]\n",
      "52 50 [D loss: 0.509318, acc.: 74.61%] [G loss: 1.106017, acc.: 23.44%]\n",
      "52 100 [D loss: 0.559014, acc.: 69.92%] [G loss: 1.088754, acc.: 28.12%]\n",
      "52 150 [D loss: 0.580248, acc.: 73.05%] [G loss: 1.053319, acc.: 27.34%]\n",
      "52 200 [D loss: 0.547568, acc.: 75.39%] [G loss: 1.126889, acc.: 21.09%]\n",
      "52 250 [D loss: 0.526041, acc.: 77.73%] [G loss: 1.055512, acc.: 21.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 300 [D loss: 0.515929, acc.: 73.44%] [G loss: 1.188671, acc.: 18.75%]\n",
      "52 350 [D loss: 0.475070, acc.: 77.73%] [G loss: 1.186671, acc.: 20.31%]\n",
      "52 400 [D loss: 0.503523, acc.: 73.44%] [G loss: 1.095903, acc.: 23.44%]\n",
      "52 450 [D loss: 0.526235, acc.: 73.83%] [G loss: 1.161411, acc.: 18.75%]\n",
      "53 0 [D loss: 0.501170, acc.: 74.22%] [G loss: 1.228190, acc.: 16.41%]\n",
      "53 50 [D loss: 0.511831, acc.: 76.17%] [G loss: 1.097444, acc.: 26.56%]\n",
      "53 100 [D loss: 0.535668, acc.: 75.39%] [G loss: 1.121582, acc.: 21.09%]\n",
      "53 150 [D loss: 0.514975, acc.: 76.56%] [G loss: 1.197478, acc.: 21.09%]\n",
      "53 200 [D loss: 0.515652, acc.: 76.56%] [G loss: 1.142121, acc.: 17.97%]\n",
      "53 250 [D loss: 0.475955, acc.: 79.69%] [G loss: 1.141556, acc.: 18.75%]\n",
      "53 300 [D loss: 0.521538, acc.: 74.22%] [G loss: 1.176307, acc.: 19.53%]\n",
      "53 350 [D loss: 0.593003, acc.: 69.14%] [G loss: 1.090773, acc.: 25.78%]\n",
      "53 400 [D loss: 0.495460, acc.: 76.95%] [G loss: 1.168342, acc.: 21.88%]\n",
      "53 450 [D loss: 0.515881, acc.: 76.95%] [G loss: 1.172455, acc.: 16.41%]\n",
      "54 0 [D loss: 0.525994, acc.: 73.44%] [G loss: 1.158634, acc.: 17.97%]\n",
      "54 50 [D loss: 0.520899, acc.: 75.00%] [G loss: 1.107192, acc.: 17.19%]\n",
      "54 100 [D loss: 0.510511, acc.: 76.56%] [G loss: 1.175887, acc.: 20.31%]\n",
      "54 150 [D loss: 0.530806, acc.: 76.17%] [G loss: 1.170911, acc.: 19.53%]\n",
      "54 200 [D loss: 0.529428, acc.: 75.39%] [G loss: 1.145272, acc.: 20.31%]\n",
      "54 250 [D loss: 0.533299, acc.: 73.44%] [G loss: 1.122751, acc.: 23.44%]\n",
      "54 300 [D loss: 0.495598, acc.: 78.52%] [G loss: 1.119549, acc.: 22.66%]\n",
      "54 350 [D loss: 0.494673, acc.: 75.78%] [G loss: 1.135036, acc.: 17.97%]\n",
      "54 400 [D loss: 0.530598, acc.: 75.78%] [G loss: 1.151263, acc.: 21.09%]\n",
      "54 450 [D loss: 0.601539, acc.: 69.53%] [G loss: 1.149618, acc.: 24.22%]\n",
      "55 0 [D loss: 0.528274, acc.: 73.05%] [G loss: 1.112828, acc.: 18.75%]\n",
      "55 50 [D loss: 0.491249, acc.: 77.34%] [G loss: 1.179591, acc.: 17.19%]\n",
      "55 100 [D loss: 0.479092, acc.: 76.17%] [G loss: 1.145416, acc.: 20.31%]\n",
      "55 150 [D loss: 0.511752, acc.: 75.39%] [G loss: 1.072687, acc.: 25.78%]\n",
      "55 200 [D loss: 0.553355, acc.: 72.27%] [G loss: 1.119691, acc.: 24.22%]\n",
      "55 250 [D loss: 0.548228, acc.: 71.09%] [G loss: 1.102541, acc.: 21.88%]\n",
      "55 300 [D loss: 0.547130, acc.: 71.88%] [G loss: 1.093480, acc.: 23.44%]\n",
      "55 350 [D loss: 0.502889, acc.: 77.73%] [G loss: 1.155825, acc.: 18.75%]\n",
      "55 400 [D loss: 0.512111, acc.: 76.56%] [G loss: 1.148695, acc.: 24.22%]\n",
      "55 450 [D loss: 0.496067, acc.: 76.56%] [G loss: 1.165595, acc.: 19.53%]\n",
      "56 0 [D loss: 0.536118, acc.: 73.44%] [G loss: 1.078699, acc.: 21.88%]\n",
      "56 50 [D loss: 0.554273, acc.: 71.48%] [G loss: 1.155218, acc.: 19.53%]\n",
      "56 100 [D loss: 0.518458, acc.: 72.66%] [G loss: 1.135484, acc.: 25.00%]\n",
      "56 150 [D loss: 0.537751, acc.: 76.17%] [G loss: 1.091371, acc.: 22.66%]\n",
      "56 200 [D loss: 0.540694, acc.: 72.66%] [G loss: 1.056039, acc.: 33.59%]\n",
      "56 250 [D loss: 0.511606, acc.: 75.39%] [G loss: 1.207701, acc.: 19.53%]\n",
      "56 300 [D loss: 0.553738, acc.: 72.27%] [G loss: 1.166316, acc.: 17.97%]\n",
      "56 350 [D loss: 0.537224, acc.: 72.27%] [G loss: 1.101513, acc.: 20.31%]\n",
      "56 400 [D loss: 0.524683, acc.: 74.22%] [G loss: 1.090317, acc.: 20.31%]\n",
      "56 450 [D loss: 0.535891, acc.: 71.48%] [G loss: 1.092984, acc.: 23.44%]\n",
      "57 0 [D loss: 0.559124, acc.: 71.48%] [G loss: 1.007425, acc.: 27.34%]\n",
      "57 50 [D loss: 0.540812, acc.: 75.39%] [G loss: 1.136910, acc.: 14.84%]\n",
      "57 100 [D loss: 0.532929, acc.: 71.48%] [G loss: 1.185114, acc.: 17.19%]\n",
      "57 150 [D loss: 0.515254, acc.: 75.78%] [G loss: 1.098205, acc.: 19.53%]\n",
      "57 200 [D loss: 0.526326, acc.: 76.95%] [G loss: 1.144183, acc.: 16.41%]\n",
      "57 250 [D loss: 0.514286, acc.: 74.22%] [G loss: 1.138507, acc.: 23.44%]\n",
      "57 300 [D loss: 0.552125, acc.: 69.53%] [G loss: 1.027807, acc.: 25.00%]\n",
      "57 350 [D loss: 0.570503, acc.: 72.27%] [G loss: 1.135439, acc.: 21.88%]\n",
      "57 400 [D loss: 0.545336, acc.: 73.44%] [G loss: 1.064337, acc.: 25.78%]\n",
      "57 450 [D loss: 0.547276, acc.: 71.88%] [G loss: 1.122909, acc.: 21.88%]\n",
      "58 0 [D loss: 0.527762, acc.: 75.00%] [G loss: 1.215991, acc.: 16.41%]\n",
      "58 50 [D loss: 0.531696, acc.: 73.83%] [G loss: 1.112870, acc.: 22.66%]\n",
      "58 100 [D loss: 0.504129, acc.: 72.27%] [G loss: 1.210679, acc.: 15.62%]\n",
      "58 150 [D loss: 0.533654, acc.: 72.66%] [G loss: 1.024989, acc.: 25.78%]\n",
      "58 200 [D loss: 0.510320, acc.: 78.52%] [G loss: 1.140844, acc.: 17.97%]\n",
      "58 250 [D loss: 0.517211, acc.: 76.17%] [G loss: 1.188388, acc.: 22.66%]\n",
      "58 300 [D loss: 0.528617, acc.: 76.56%] [G loss: 1.100602, acc.: 24.22%]\n",
      "58 350 [D loss: 0.541085, acc.: 72.66%] [G loss: 1.116996, acc.: 25.78%]\n",
      "58 400 [D loss: 0.542148, acc.: 71.88%] [G loss: 1.082123, acc.: 25.00%]\n",
      "58 450 [D loss: 0.547874, acc.: 74.61%] [G loss: 1.136672, acc.: 18.75%]\n",
      "59 0 [D loss: 0.550563, acc.: 72.27%] [G loss: 1.079968, acc.: 21.88%]\n",
      "59 50 [D loss: 0.514282, acc.: 75.39%] [G loss: 1.126523, acc.: 19.53%]\n",
      "59 100 [D loss: 0.548738, acc.: 71.48%] [G loss: 1.147450, acc.: 20.31%]\n",
      "59 150 [D loss: 0.558780, acc.: 72.27%] [G loss: 1.143888, acc.: 19.53%]\n",
      "59 200 [D loss: 0.542909, acc.: 74.22%] [G loss: 1.103157, acc.: 18.75%]\n",
      "59 250 [D loss: 0.506587, acc.: 75.78%] [G loss: 1.130116, acc.: 19.53%]\n",
      "59 300 [D loss: 0.481261, acc.: 78.91%] [G loss: 1.208139, acc.: 21.88%]\n",
      "59 350 [D loss: 0.573053, acc.: 71.48%] [G loss: 1.086171, acc.: 21.88%]\n",
      "59 400 [D loss: 0.547601, acc.: 74.22%] [G loss: 1.163480, acc.: 15.62%]\n",
      "59 450 [D loss: 0.535814, acc.: 74.61%] [G loss: 1.097384, acc.: 23.44%]\n",
      "60 0 [D loss: 0.530494, acc.: 70.70%] [G loss: 1.116518, acc.: 21.09%]\n",
      "60 50 [D loss: 0.534389, acc.: 74.22%] [G loss: 1.106958, acc.: 20.31%]\n",
      "60 100 [D loss: 0.529360, acc.: 74.61%] [G loss: 1.114724, acc.: 21.09%]\n",
      "60 150 [D loss: 0.544044, acc.: 75.00%] [G loss: 1.091009, acc.: 25.00%]\n",
      "60 200 [D loss: 0.541797, acc.: 73.05%] [G loss: 1.026978, acc.: 28.12%]\n",
      "60 250 [D loss: 0.568240, acc.: 71.48%] [G loss: 1.090994, acc.: 24.22%]\n",
      "60 300 [D loss: 0.555290, acc.: 70.70%] [G loss: 1.134015, acc.: 23.44%]\n",
      "60 350 [D loss: 0.489067, acc.: 80.86%] [G loss: 1.246641, acc.: 11.72%]\n",
      "60 400 [D loss: 0.510753, acc.: 75.78%] [G loss: 1.155828, acc.: 18.75%]\n",
      "60 450 [D loss: 0.532296, acc.: 74.61%] [G loss: 1.110853, acc.: 17.19%]\n",
      "61 0 [D loss: 0.531498, acc.: 73.44%] [G loss: 1.131392, acc.: 22.66%]\n",
      "61 50 [D loss: 0.515806, acc.: 73.83%] [G loss: 1.054732, acc.: 22.66%]\n",
      "61 100 [D loss: 0.517290, acc.: 74.22%] [G loss: 1.134324, acc.: 21.09%]\n",
      "61 150 [D loss: 0.537677, acc.: 73.05%] [G loss: 1.173960, acc.: 21.88%]\n",
      "61 200 [D loss: 0.472079, acc.: 76.95%] [G loss: 1.150656, acc.: 23.44%]\n",
      "61 250 [D loss: 0.543366, acc.: 73.44%] [G loss: 1.076369, acc.: 21.09%]\n",
      "61 300 [D loss: 0.519271, acc.: 73.05%] [G loss: 1.132002, acc.: 18.75%]\n",
      "61 350 [D loss: 0.534304, acc.: 72.27%] [G loss: 1.047439, acc.: 22.66%]\n",
      "61 400 [D loss: 0.530525, acc.: 76.95%] [G loss: 1.093125, acc.: 24.22%]\n",
      "61 450 [D loss: 0.524055, acc.: 75.39%] [G loss: 1.139929, acc.: 21.09%]\n",
      "62 0 [D loss: 0.555832, acc.: 73.05%] [G loss: 1.100732, acc.: 25.00%]\n",
      "62 50 [D loss: 0.472223, acc.: 78.91%] [G loss: 1.180633, acc.: 12.50%]\n",
      "62 100 [D loss: 0.488574, acc.: 77.34%] [G loss: 1.206110, acc.: 15.62%]\n",
      "62 150 [D loss: 0.474854, acc.: 79.30%] [G loss: 1.228475, acc.: 14.06%]\n",
      "62 200 [D loss: 0.485675, acc.: 78.52%] [G loss: 1.154866, acc.: 18.75%]\n",
      "62 250 [D loss: 0.569007, acc.: 75.39%] [G loss: 1.064529, acc.: 25.78%]\n",
      "62 300 [D loss: 0.547099, acc.: 72.66%] [G loss: 1.127988, acc.: 24.22%]\n",
      "62 350 [D loss: 0.516406, acc.: 73.83%] [G loss: 1.067767, acc.: 23.44%]\n",
      "62 400 [D loss: 0.497725, acc.: 75.00%] [G loss: 1.185791, acc.: 18.75%]\n",
      "62 450 [D loss: 0.512439, acc.: 76.17%] [G loss: 1.128363, acc.: 17.97%]\n",
      "63 0 [D loss: 0.507490, acc.: 77.34%] [G loss: 1.163734, acc.: 14.06%]\n",
      "63 50 [D loss: 0.545029, acc.: 70.31%] [G loss: 1.094689, acc.: 21.09%]\n",
      "63 100 [D loss: 0.521093, acc.: 78.52%] [G loss: 1.103260, acc.: 17.97%]\n",
      "63 150 [D loss: 0.493584, acc.: 77.34%] [G loss: 1.142709, acc.: 17.97%]\n",
      "63 200 [D loss: 0.499454, acc.: 78.52%] [G loss: 1.144376, acc.: 19.53%]\n",
      "63 250 [D loss: 0.518922, acc.: 75.78%] [G loss: 1.152116, acc.: 20.31%]\n",
      "63 300 [D loss: 0.582258, acc.: 69.53%] [G loss: 1.066930, acc.: 25.00%]\n",
      "63 350 [D loss: 0.510750, acc.: 77.34%] [G loss: 1.086465, acc.: 21.88%]\n",
      "63 400 [D loss: 0.545822, acc.: 73.44%] [G loss: 1.151964, acc.: 19.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 450 [D loss: 0.558838, acc.: 71.09%] [G loss: 1.082309, acc.: 17.97%]\n",
      "64 0 [D loss: 0.523711, acc.: 74.61%] [G loss: 1.116993, acc.: 18.75%]\n",
      "64 50 [D loss: 0.513154, acc.: 73.05%] [G loss: 1.083929, acc.: 26.56%]\n",
      "64 100 [D loss: 0.515332, acc.: 76.56%] [G loss: 1.107011, acc.: 22.66%]\n",
      "64 150 [D loss: 0.551476, acc.: 71.09%] [G loss: 1.075144, acc.: 22.66%]\n",
      "64 200 [D loss: 0.556912, acc.: 69.53%] [G loss: 1.138586, acc.: 17.97%]\n",
      "64 250 [D loss: 0.573630, acc.: 70.31%] [G loss: 1.106667, acc.: 27.34%]\n",
      "64 300 [D loss: 0.536184, acc.: 73.05%] [G loss: 1.138114, acc.: 16.41%]\n",
      "64 350 [D loss: 0.521057, acc.: 75.00%] [G loss: 1.110577, acc.: 23.44%]\n",
      "64 400 [D loss: 0.559045, acc.: 75.00%] [G loss: 1.060055, acc.: 28.12%]\n",
      "64 450 [D loss: 0.557193, acc.: 72.66%] [G loss: 1.082734, acc.: 23.44%]\n",
      "65 0 [D loss: 0.496901, acc.: 77.73%] [G loss: 1.156622, acc.: 21.09%]\n",
      "65 50 [D loss: 0.542533, acc.: 72.66%] [G loss: 1.044188, acc.: 25.00%]\n",
      "65 100 [D loss: 0.559922, acc.: 66.80%] [G loss: 1.127904, acc.: 28.91%]\n",
      "65 150 [D loss: 0.538914, acc.: 74.22%] [G loss: 1.086961, acc.: 21.09%]\n",
      "65 200 [D loss: 0.575952, acc.: 71.09%] [G loss: 1.110002, acc.: 24.22%]\n",
      "65 250 [D loss: 0.519976, acc.: 74.61%] [G loss: 1.155315, acc.: 24.22%]\n",
      "65 300 [D loss: 0.508223, acc.: 75.78%] [G loss: 1.147525, acc.: 15.62%]\n",
      "65 350 [D loss: 0.504045, acc.: 79.69%] [G loss: 1.189171, acc.: 17.97%]\n",
      "65 400 [D loss: 0.503593, acc.: 78.12%] [G loss: 1.163509, acc.: 20.31%]\n",
      "65 450 [D loss: 0.568920, acc.: 68.36%] [G loss: 1.029137, acc.: 28.91%]\n",
      "66 0 [D loss: 0.514960, acc.: 76.56%] [G loss: 1.144606, acc.: 17.97%]\n",
      "66 50 [D loss: 0.557798, acc.: 71.48%] [G loss: 1.088192, acc.: 21.09%]\n",
      "66 100 [D loss: 0.480398, acc.: 79.30%] [G loss: 1.128082, acc.: 18.75%]\n",
      "66 150 [D loss: 0.535348, acc.: 73.44%] [G loss: 1.169858, acc.: 19.53%]\n",
      "66 200 [D loss: 0.526385, acc.: 77.73%] [G loss: 1.085570, acc.: 21.88%]\n",
      "66 250 [D loss: 0.528124, acc.: 71.88%] [G loss: 1.113410, acc.: 19.53%]\n",
      "66 300 [D loss: 0.507563, acc.: 74.22%] [G loss: 1.140660, acc.: 19.53%]\n",
      "66 350 [D loss: 0.542445, acc.: 72.66%] [G loss: 1.148481, acc.: 19.53%]\n",
      "66 400 [D loss: 0.481383, acc.: 76.56%] [G loss: 1.147369, acc.: 20.31%]\n",
      "66 450 [D loss: 0.545785, acc.: 71.88%] [G loss: 1.135233, acc.: 25.78%]\n",
      "67 0 [D loss: 0.503828, acc.: 75.00%] [G loss: 1.157907, acc.: 16.41%]\n",
      "67 50 [D loss: 0.502457, acc.: 77.34%] [G loss: 1.084103, acc.: 25.00%]\n",
      "67 100 [D loss: 0.552732, acc.: 70.70%] [G loss: 1.050221, acc.: 25.00%]\n",
      "67 150 [D loss: 0.515117, acc.: 76.17%] [G loss: 1.253932, acc.: 14.06%]\n",
      "67 200 [D loss: 0.501239, acc.: 73.44%] [G loss: 1.091188, acc.: 22.66%]\n",
      "67 250 [D loss: 0.514990, acc.: 73.44%] [G loss: 1.187196, acc.: 23.44%]\n",
      "67 300 [D loss: 0.513947, acc.: 78.12%] [G loss: 1.149071, acc.: 17.97%]\n",
      "67 350 [D loss: 0.534197, acc.: 75.78%] [G loss: 1.186615, acc.: 17.97%]\n",
      "67 400 [D loss: 0.558617, acc.: 72.27%] [G loss: 1.147349, acc.: 21.09%]\n",
      "67 450 [D loss: 0.516450, acc.: 77.34%] [G loss: 1.216565, acc.: 19.53%]\n",
      "68 0 [D loss: 0.522409, acc.: 75.00%] [G loss: 1.102301, acc.: 21.09%]\n",
      "68 50 [D loss: 0.496867, acc.: 75.00%] [G loss: 1.124096, acc.: 24.22%]\n",
      "68 100 [D loss: 0.561193, acc.: 69.92%] [G loss: 1.065275, acc.: 26.56%]\n",
      "68 150 [D loss: 0.532473, acc.: 74.22%] [G loss: 1.139880, acc.: 21.88%]\n",
      "68 200 [D loss: 0.549885, acc.: 73.83%] [G loss: 1.133921, acc.: 23.44%]\n",
      "68 250 [D loss: 0.533753, acc.: 73.44%] [G loss: 1.073378, acc.: 20.31%]\n",
      "68 300 [D loss: 0.476432, acc.: 79.69%] [G loss: 1.153685, acc.: 17.97%]\n",
      "68 350 [D loss: 0.520591, acc.: 76.56%] [G loss: 1.086581, acc.: 23.44%]\n",
      "68 400 [D loss: 0.552671, acc.: 75.00%] [G loss: 1.054277, acc.: 25.00%]\n",
      "68 450 [D loss: 0.545130, acc.: 71.88%] [G loss: 1.118977, acc.: 25.00%]\n",
      "69 0 [D loss: 0.560618, acc.: 68.36%] [G loss: 1.092119, acc.: 18.75%]\n",
      "69 50 [D loss: 0.552486, acc.: 70.31%] [G loss: 1.111832, acc.: 19.53%]\n",
      "69 100 [D loss: 0.523009, acc.: 76.56%] [G loss: 1.092102, acc.: 25.00%]\n",
      "69 150 [D loss: 0.540191, acc.: 76.17%] [G loss: 1.120725, acc.: 23.44%]\n",
      "69 200 [D loss: 0.544415, acc.: 74.22%] [G loss: 1.192822, acc.: 21.09%]\n",
      "69 250 [D loss: 0.537265, acc.: 75.39%] [G loss: 1.151781, acc.: 13.28%]\n",
      "69 300 [D loss: 0.548171, acc.: 70.31%] [G loss: 1.103054, acc.: 23.44%]\n",
      "69 350 [D loss: 0.526614, acc.: 73.05%] [G loss: 1.158367, acc.: 17.97%]\n",
      "69 400 [D loss: 0.532918, acc.: 74.22%] [G loss: 1.129012, acc.: 21.09%]\n",
      "69 450 [D loss: 0.471867, acc.: 76.56%] [G loss: 1.158132, acc.: 17.19%]\n",
      "70 0 [D loss: 0.537447, acc.: 73.44%] [G loss: 1.155978, acc.: 21.09%]\n",
      "70 50 [D loss: 0.513582, acc.: 75.39%] [G loss: 1.128420, acc.: 18.75%]\n",
      "70 100 [D loss: 0.498249, acc.: 76.95%] [G loss: 1.153398, acc.: 20.31%]\n",
      "70 150 [D loss: 0.545910, acc.: 70.70%] [G loss: 1.083124, acc.: 25.00%]\n",
      "70 200 [D loss: 0.529958, acc.: 71.88%] [G loss: 1.058174, acc.: 22.66%]\n",
      "70 250 [D loss: 0.493131, acc.: 78.52%] [G loss: 1.159254, acc.: 24.22%]\n",
      "70 300 [D loss: 0.538231, acc.: 76.56%] [G loss: 1.142124, acc.: 15.62%]\n",
      "70 350 [D loss: 0.523337, acc.: 73.44%] [G loss: 1.139995, acc.: 16.41%]\n",
      "70 400 [D loss: 0.507200, acc.: 73.83%] [G loss: 1.126605, acc.: 25.00%]\n",
      "70 450 [D loss: 0.509142, acc.: 78.52%] [G loss: 1.191937, acc.: 16.41%]\n",
      "71 0 [D loss: 0.530143, acc.: 73.44%] [G loss: 1.126719, acc.: 21.09%]\n",
      "71 50 [D loss: 0.540282, acc.: 72.66%] [G loss: 1.040707, acc.: 25.00%]\n",
      "71 100 [D loss: 0.541981, acc.: 73.83%] [G loss: 1.093959, acc.: 20.31%]\n",
      "71 150 [D loss: 0.537152, acc.: 74.22%] [G loss: 1.127639, acc.: 18.75%]\n",
      "71 200 [D loss: 0.502914, acc.: 76.17%] [G loss: 1.192937, acc.: 16.41%]\n",
      "71 250 [D loss: 0.542665, acc.: 72.66%] [G loss: 1.089153, acc.: 28.91%]\n",
      "71 300 [D loss: 0.567824, acc.: 69.53%] [G loss: 1.043367, acc.: 28.12%]\n",
      "71 350 [D loss: 0.526087, acc.: 76.56%] [G loss: 1.190084, acc.: 14.06%]\n",
      "71 400 [D loss: 0.528447, acc.: 76.56%] [G loss: 1.125547, acc.: 22.66%]\n",
      "71 450 [D loss: 0.536216, acc.: 74.61%] [G loss: 1.214184, acc.: 15.62%]\n",
      "72 0 [D loss: 0.550406, acc.: 71.48%] [G loss: 1.114530, acc.: 22.66%]\n",
      "72 50 [D loss: 0.569320, acc.: 70.70%] [G loss: 1.112458, acc.: 21.09%]\n",
      "72 100 [D loss: 0.519844, acc.: 77.73%] [G loss: 1.103903, acc.: 24.22%]\n",
      "72 150 [D loss: 0.528660, acc.: 73.05%] [G loss: 1.065303, acc.: 26.56%]\n",
      "72 200 [D loss: 0.539392, acc.: 71.88%] [G loss: 1.057142, acc.: 26.56%]\n",
      "72 250 [D loss: 0.525367, acc.: 76.56%] [G loss: 1.147146, acc.: 21.88%]\n",
      "72 300 [D loss: 0.505321, acc.: 75.39%] [G loss: 1.089714, acc.: 23.44%]\n",
      "72 350 [D loss: 0.555006, acc.: 68.36%] [G loss: 1.130208, acc.: 17.19%]\n",
      "72 400 [D loss: 0.499531, acc.: 76.95%] [G loss: 1.176531, acc.: 15.62%]\n",
      "72 450 [D loss: 0.530456, acc.: 69.14%] [G loss: 1.092954, acc.: 23.44%]\n",
      "73 0 [D loss: 0.511263, acc.: 75.00%] [G loss: 1.062627, acc.: 24.22%]\n",
      "73 50 [D loss: 0.561726, acc.: 71.09%] [G loss: 1.048803, acc.: 21.88%]\n",
      "73 100 [D loss: 0.482246, acc.: 78.91%] [G loss: 1.167365, acc.: 17.19%]\n",
      "73 150 [D loss: 0.505472, acc.: 76.56%] [G loss: 1.121679, acc.: 17.97%]\n",
      "73 200 [D loss: 0.463386, acc.: 79.69%] [G loss: 1.189232, acc.: 13.28%]\n",
      "73 250 [D loss: 0.543489, acc.: 73.83%] [G loss: 1.096756, acc.: 23.44%]\n",
      "73 300 [D loss: 0.545094, acc.: 73.83%] [G loss: 1.088220, acc.: 20.31%]\n",
      "73 350 [D loss: 0.533142, acc.: 75.00%] [G loss: 1.132136, acc.: 18.75%]\n",
      "73 400 [D loss: 0.522818, acc.: 72.66%] [G loss: 1.106342, acc.: 17.97%]\n",
      "73 450 [D loss: 0.534910, acc.: 73.83%] [G loss: 1.109783, acc.: 23.44%]\n",
      "74 0 [D loss: 0.485773, acc.: 79.69%] [G loss: 1.090502, acc.: 18.75%]\n",
      "74 50 [D loss: 0.515373, acc.: 75.00%] [G loss: 1.117946, acc.: 21.09%]\n",
      "74 100 [D loss: 0.530953, acc.: 70.70%] [G loss: 1.080449, acc.: 26.56%]\n",
      "74 150 [D loss: 0.490360, acc.: 73.44%] [G loss: 1.236521, acc.: 16.41%]\n",
      "74 200 [D loss: 0.563284, acc.: 69.92%] [G loss: 1.091035, acc.: 24.22%]\n",
      "74 250 [D loss: 0.560027, acc.: 69.92%] [G loss: 1.052089, acc.: 28.12%]\n",
      "74 300 [D loss: 0.541347, acc.: 76.56%] [G loss: 1.128461, acc.: 21.09%]\n",
      "74 350 [D loss: 0.550866, acc.: 71.48%] [G loss: 1.027389, acc.: 31.25%]\n",
      "74 400 [D loss: 0.489540, acc.: 78.52%] [G loss: 1.120058, acc.: 21.09%]\n",
      "74 450 [D loss: 0.528849, acc.: 72.66%] [G loss: 1.106541, acc.: 19.53%]\n",
      "75 0 [D loss: 0.502338, acc.: 78.52%] [G loss: 1.174536, acc.: 15.62%]\n",
      "75 50 [D loss: 0.518517, acc.: 76.95%] [G loss: 1.193977, acc.: 17.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 100 [D loss: 0.540562, acc.: 73.05%] [G loss: 1.102075, acc.: 21.09%]\n",
      "75 150 [D loss: 0.560738, acc.: 73.05%] [G loss: 1.134668, acc.: 19.53%]\n",
      "75 200 [D loss: 0.524836, acc.: 74.61%] [G loss: 1.152559, acc.: 22.66%]\n",
      "75 250 [D loss: 0.536612, acc.: 75.39%] [G loss: 1.047552, acc.: 27.34%]\n",
      "75 300 [D loss: 0.510277, acc.: 75.39%] [G loss: 1.122161, acc.: 22.66%]\n",
      "75 350 [D loss: 0.511582, acc.: 76.56%] [G loss: 1.139919, acc.: 19.53%]\n",
      "75 400 [D loss: 0.544493, acc.: 71.09%] [G loss: 1.009753, acc.: 28.91%]\n",
      "75 450 [D loss: 0.507205, acc.: 75.39%] [G loss: 1.090768, acc.: 22.66%]\n",
      "76 0 [D loss: 0.606349, acc.: 72.27%] [G loss: 1.041965, acc.: 23.44%]\n",
      "76 50 [D loss: 0.538095, acc.: 71.48%] [G loss: 1.131898, acc.: 25.00%]\n",
      "76 100 [D loss: 0.537540, acc.: 73.44%] [G loss: 1.168484, acc.: 19.53%]\n",
      "76 150 [D loss: 0.504379, acc.: 77.34%] [G loss: 1.075953, acc.: 23.44%]\n",
      "76 200 [D loss: 0.510217, acc.: 72.27%] [G loss: 1.180113, acc.: 21.88%]\n",
      "76 250 [D loss: 0.509418, acc.: 74.61%] [G loss: 1.158452, acc.: 25.00%]\n",
      "76 300 [D loss: 0.512593, acc.: 76.95%] [G loss: 1.165870, acc.: 20.31%]\n",
      "76 350 [D loss: 0.525949, acc.: 76.95%] [G loss: 1.128261, acc.: 19.53%]\n",
      "76 400 [D loss: 0.561975, acc.: 72.27%] [G loss: 1.103319, acc.: 26.56%]\n",
      "76 450 [D loss: 0.503114, acc.: 73.44%] [G loss: 1.155790, acc.: 21.09%]\n",
      "77 0 [D loss: 0.507540, acc.: 74.61%] [G loss: 1.129462, acc.: 23.44%]\n",
      "77 50 [D loss: 0.544377, acc.: 69.53%] [G loss: 1.161422, acc.: 25.78%]\n",
      "77 100 [D loss: 0.486667, acc.: 77.34%] [G loss: 1.136529, acc.: 20.31%]\n",
      "77 150 [D loss: 0.522846, acc.: 72.66%] [G loss: 1.128477, acc.: 18.75%]\n",
      "77 200 [D loss: 0.519820, acc.: 75.78%] [G loss: 1.183571, acc.: 20.31%]\n",
      "77 250 [D loss: 0.557671, acc.: 71.09%] [G loss: 1.197894, acc.: 18.75%]\n",
      "77 300 [D loss: 0.524655, acc.: 74.61%] [G loss: 1.089042, acc.: 26.56%]\n",
      "77 350 [D loss: 0.497122, acc.: 75.39%] [G loss: 1.177517, acc.: 19.53%]\n",
      "77 400 [D loss: 0.506697, acc.: 76.56%] [G loss: 1.074301, acc.: 20.31%]\n",
      "77 450 [D loss: 0.535383, acc.: 75.39%] [G loss: 1.083851, acc.: 21.09%]\n",
      "78 0 [D loss: 0.499302, acc.: 75.78%] [G loss: 1.131290, acc.: 17.97%]\n",
      "78 50 [D loss: 0.522021, acc.: 72.66%] [G loss: 1.084161, acc.: 22.66%]\n",
      "78 100 [D loss: 0.534399, acc.: 74.61%] [G loss: 1.139342, acc.: 24.22%]\n",
      "78 150 [D loss: 0.530257, acc.: 76.17%] [G loss: 1.081527, acc.: 23.44%]\n",
      "78 200 [D loss: 0.517292, acc.: 76.17%] [G loss: 1.156994, acc.: 21.09%]\n",
      "78 250 [D loss: 0.515173, acc.: 75.39%] [G loss: 1.065498, acc.: 24.22%]\n",
      "78 300 [D loss: 0.498535, acc.: 75.39%] [G loss: 1.185628, acc.: 14.06%]\n",
      "78 350 [D loss: 0.526564, acc.: 75.00%] [G loss: 1.101754, acc.: 18.75%]\n",
      "78 400 [D loss: 0.547601, acc.: 73.44%] [G loss: 1.128785, acc.: 21.09%]\n",
      "78 450 [D loss: 0.498084, acc.: 77.73%] [G loss: 1.104412, acc.: 21.88%]\n",
      "79 0 [D loss: 0.519743, acc.: 73.05%] [G loss: 1.167752, acc.: 17.19%]\n",
      "79 50 [D loss: 0.496536, acc.: 78.91%] [G loss: 1.128212, acc.: 22.66%]\n",
      "79 100 [D loss: 0.518910, acc.: 74.22%] [G loss: 1.097689, acc.: 21.09%]\n",
      "79 150 [D loss: 0.562267, acc.: 71.48%] [G loss: 1.052633, acc.: 22.66%]\n",
      "79 200 [D loss: 0.526196, acc.: 71.88%] [G loss: 1.109477, acc.: 28.12%]\n",
      "79 250 [D loss: 0.533283, acc.: 75.00%] [G loss: 1.127856, acc.: 21.88%]\n",
      "79 300 [D loss: 0.577197, acc.: 69.92%] [G loss: 1.158805, acc.: 17.19%]\n",
      "79 350 [D loss: 0.495298, acc.: 78.12%] [G loss: 1.052229, acc.: 18.75%]\n",
      "79 400 [D loss: 0.521738, acc.: 75.39%] [G loss: 1.125271, acc.: 17.19%]\n",
      "79 450 [D loss: 0.518905, acc.: 75.00%] [G loss: 1.148393, acc.: 19.53%]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=80, batch_size=128, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
